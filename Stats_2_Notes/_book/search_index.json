[["index.html", "Course Introduction Chapter 1 Introduction to R and Demography 1.1 Why R? 1.2 Who is this book for?", " Course Introduction Corey Sparks, PhD 2021-11-19 Chapter 1 Introduction to R and Demography 1.1 Why R? Ive used R for twenty years. I was also trained in SPSS and SAS along the way, by various mentors. Some tried to get me to learn more general purpose languages like Delphi (of all things) or Perl, or Basic, and Ive been chastised for not knowing the depths of Python, but R presents a nimble and rigorous platform to do demography. My top three reasons for teaching and using R are: Its free - This is important, because, why should we pass along more costs to people, especially our students? This also make R code accessible to people, worldwide. Its the hotbed of methodological development. The R ecosystem has thousands of packages that represent the bleeding edge of data analysis, visualization and data science. This makes R attractive because it can pivot quickly to adopt new methods, which often lag in their development in other environments. It has a supportive community of users. While there are some debates over how friendly some R users are to new users, overall, after spending 20 years in the R community, Ive personally assisted hundreds of users, and been personally helped by many others. The open source nature of R lends itself to sharing of ideas and collaboration between users. 1.1.1 My assumptions in this book In statistics we always make assumptions, often these are wrong, but we adapt to our mistakes daily. My assumptions about who is reading this book are: You are interested in learning more about R. You are likely a student or professional interested in demography or population research. You have likely been exposed to other statistical platforms and are curious about R, in conjunction with 1 and 2 above. You may be an avid R user from another strange and exotic discipline, but are interested in how demographers do research. You want to see how to do things instead of being bombarded with theoretical and often unnecessary gate-keeping mathematical treatments of statistical models. I think if any of these assumptions are true, youre in the right place. That being said, this book is not a review of all of statistics, nor is it an encyclopedic coverage of the R language and ecosystem. I image the latter being on the same scale of hopelessness as the search for the Holy Grail or the fountain of youth. People have died for such fool hearty quests, Im not falling on my sword here folks. 1.2 Who is this book for? This book has come from several courses that I teach in our Applied Demography program at the University of Texas at San Antonio. MORE NEEDED "],["hello-bookdown.html", "Chapter 2 Hello bookdown 2.1 A section", " Chapter 2 Hello bookdown All chapters start with a first-level heading followed by your chapter title, like the line above. There should be only one first-level heading (#) per .Rmd file. 2.1 A section All chapter sections start with a second-level (##) or higher heading followed by your section title, like the sections above and below here. You can have as many as you want within a chapter. An unnumbered section Chapters and sections are numbered by default. To un-number a heading, add a {.unnumbered} or the shorter {-} at the end of the heading, like in this section. "],["introduction-to-r.html", "Chapter 3 Introduction to R 3.1 Welcome to R. 3.2 R and Rstudio 3.3 Introduction to Rstudio 3.4 Getting help in R 3.5 R packages 3.6 Your R user environment 3.7 Some Simple R examples 3.8 Variables and objects 3.9 Variable types 3.10 Dataframes 3.11 Real data example 3.12 Basic Descriptive analysis of data 3.13 The tidyverse 3.14 Basic ggplot 3.15 Chapter summary 3.16 References", " Chapter 3 Introduction to R This chapter is devoted to introducing R to new users. R was first implemented in the early 1990s by Robert Gentleman and Ross Ihaka, both faculty members at the University of Auckland. It is an open source software system that specializes in statistical analysis and graphics. R is its own programming language and the R ecosystem includes over 18,000 user-contributed additions to the software, known as packages. 3.1 Welcome to R. If youre coming to R from SAS, there is no data step. There are no procs. The SAS and R book Kleinman and Horton (2014) is very useful for going between the two programs. If youre coming from SPSS and youve been using the button clicking method, be prepared for a steep learning curve. If youve been writing syntax in SPSS, youre at least used to having to code. Theres a good book for SAS and SPSS users by Bob Meunchen at the Univ. of Tennessee here, which may be of some help. Stata users have fewer official publications at their fingertips to ease the transition to R, but I have always thought that the two were very similar. If you search the internet for information related to R and Stata, you will find a myriad of (somewhat dated) blog posts on which one is better for data science, how to get started with either and so forth. What is really needed is a text similar to Kleinman and Horton (2014) which acts as a crosswalk between the two programs. 3.2 R and Rstudio The Rgui is the base version of R, but is not very good to program in. Rstudio is much better, as it gives you a true integrated development environment (IDE), where you can write code in one window, see results in others, see locations of files, and see objects youve created. To get started, you should download the R installer for your operating system. Windows and Mac have installer files, and Linux users will install R using their preferred package manager. Download R from CRAN. If youre on Windows, I would also highly recommend you install Rtools, because it gives you c++ and Fortran compilers, which many packages need to be installed. Rstudio can be downloaded for free here. I would recommend installing the base R program from CRAN first then (for Windows users) install Rtools, then install Rstudio, in that order. 3.3 Introduction to Rstudio Again, each operating system has its own binary for Rstudio, so pick the one that matches your operating system. Rstudio typically has 4 sub-windows open at any given time. Rstudio is an open source Integrated Development Environment (IDE) for R. It is a much better interface for using R because it allows you to write code in multiple languages, navigate your computers files, and see your output in a very nice single place. The Rstudio IDE has several components that we will explore. 3.3.1 Code window/Source editor pane This is where you write your R code. You can write R code in a few different file types (more on this later), but the basic one is an R script, with file extension .R The code window allows you to write and execute your code one line at a time, or to run an entire script at once. I use this to develop new code and when I want to test if things work (a VERY common exercise when writing any code). To run a single line of code, put your cursor on the line and hit Ctrl-Enter (on Mac CMD-Enter also does this) To run multiple lines of code, highlight the lines you want to run and do the same thing 3.3.2 Console Pane This is where most of your non-graphical output will be shown. Any numeric output will appear here, as well as any warnings or error messages. In R a warning doesnt necessarily mean something went wrong, its just Rs polite way of telling you to pay attention. An Error means something did go wrong. This is often because you left off a ) or a, sometimes because you misspelled something. I routinely spell length as lenght which causes R to print an error message. If you see an error, dont worry, R will print some kind of message telling you what went wrong. Rs output is in plain text, although we can produce much prettier output using other output methods, and well talk more about that later. You can type commands or code into the console as well, and youll immediately get the result, versus if you write it in the Source/Code window, you have to run it to see the result. I will often work in the console when I want to get fast answers, meaning little checks that I will often do to see the value of something. 3.3.3 Environment or Workspace browser pane The R environment is where any object you create is stored. In R, anything you read in or create with your code is called an object, and R is said to be an object oriented programming language. Depending on the type of object something is, you may be able to click on the object in the environment and see more about it. For instance if the object is a data frame, R will open it in a viewer where you can explore it like a spreadsheet, and sort and filter it as well. Other objects may not do anything when you click on them. There is also a useful History tab here that shows you recently executed lines of code from the console or the code pane. 3.3.4 Files/Output/Help pane The files and output area is where you can interact with files on your local computer, such as data files or code files, or images that R can open. This area also has a plots window that will display plots you create in R either via typing directly into the console or by running a line(s) of code from the source/code pane. There is also a very valuable part of this pane that lets you access the help system in R. If you are either looking for something, or you just want to explore the functions, you can get access to all of this here. 3.3.5 R file types .R files R uses a basic text file with the .R extension. This type of file is useful if youre going to write a function or do some analysis and dont want to have formatted output or text. You can use these files for everything, but they are limited in their ability to produce reports and formatted output, so I recommend people work with R Markdown files instead. .Rmd files Rstudio uses a form of the markdown formatting language, called R Markdown, for creating formatted documents that include code, tables, figures and statistical output. This book is written in R Markdown! R Markdown is nice for lots of reasons, such as the ability to insert latex equations into documents. \\[ {y_i \\sim Normal (x&#39;\\beta, \\sigma_2)} \\] or to include output tables directly into a document: library(broom) library(pander) fit &lt;- lm(imr~tfr+pcturban+pctlt15_2018+pctwomcontra_all, data = prb) pander(broom::tidy(fit)) term estimate std.error statistic p.value (Intercept) 6.209 6.551 0.9478 0.3446 tfr 3.392 2.006 1.691 0.09274 pcturban -0.0923 0.04553 -2.028 0.04425 pctlt15_2018 0.8699 0.2441 3.564 0.0004798 pctwomcontra_all -0.2114 0.06018 -3.512 0.0005757 This allows you to make tables in Rmarkdown without having to do non-repeatable tasks in Word or some other program. You can basically do your entire analysis, or a sideshow for a presentation, or an entire paper, including bibliography, in Rstudio. 3.3.6 R projects Rstudio allows you to create a R project, which basically sets up a specific location to store R code for a given project you may be doing. For instance, this book is a single R project, which helps me organize all the chapters, bibliographies, figures, etc. R projects also allow you to use version control, including Git and SVN, to collaborate and share code and data with others. 3.3.7 R data files R allows you to read and write its own native data formats, as well as read and write text formatted files and data files from other statistical software packages. Two native R data formats are .rds and .rdata formats. .rds files allow you to save a single R object to an external files, while .rdata files allow you to save one or more objects to a file. Here is a short example of doing this, where I create 2 vectors, x and y and save them. x &lt;- c(1, 2,3) y &lt;- c(4, 5, 6) saveRDS(x, file=&quot;~/x.rds&quot;) save(list=c(&quot;x&quot;,&quot;y&quot;), file=&quot;xy.rdata&quot;) I can also load these into R again: readRDS(file = &quot;~/x.rds&quot;) ## [1] 1 2 3 load(&quot;xy.rdata&quot;) Standard methods for importing text data such as comma separated value or tab delimited files can be read into R using read.csv() or read.table() and similar writing functions are available. To read in a dataset from another statistical package, I recommend using the haven package. It allows you to read and write SAS (both sas7bdat and xpt files), Stata, SPSS (both .por and .sav files). For example, here I write out a dataframe containing x and y from above to a SAS version 7 file: xy &lt;- data.frame(x = x, y = y) xy ## x y ## 1 1 4 ## 2 2 5 ## 3 3 6 library(haven) write_sas(data = xy, path = &quot;~/xy.sas7bdat&quot;) I will describe dataframes more later in the chapter. R also has packages for reading/writing such data formats as JSON, ESRI Shapefiles, Excel spreadsheets, Google Spreadsheets, DBF files, in addition to tools for connecting to SQL databases, and for interfacing with other statistics packages, such as Mplus, OpenBUGS, WinBUGS and various Geographic Information Systems. 3.4 Getting help in R I wish I had a nickel for every time I ran into a problem trying to do something in R, that would be a lot of nickles. Here are some good tips for finding help in R: If you know the name of a function you want to use, but just need help using it, try ? ?lm If you need to find a function to do something, try ?? ??\"linear model\" You can also search the history of other R users questions by tapping into the RSiteSearch website, which is an archive of user questions to the R list serve. This can be used by tying RSiteSearch() RSiteSearch(\"heteroskedasticity\") Speaking of which, there are multiple R user email list serves that you can ask questions to, or subscribe to daily digests from. These typically want an example of what youre trying to do, referred to as a reproducible example. I wish I also had nickles for each question Ive asked and answered on these forums. A good source for all things programming is the statistics branch of Stack Exchange, which has lots of contributed questions and answers, although many answers are either very snarky or wrong or for an old version of a library, so caveat emptor. Your local R guru or R user group. You would be surprised at how many people are R users, there may be one just down the hall, or in the cubicle next door. I relish the opportunity to talk to other R users, mostly because, even though Ive used R for more than 20 years, I still learn so much by talking to others about how they use R. Lastly, I want to be clear that there are often more than one way to do everything in R. Simple things like reading and writing a CSV data file can be accomplished by any of a handful of different functions found in different packages. If someone tells you that there is only one way to do something, they are usually wrong in such a statement, regarding R at least. 3.5 R packages R uses packages to store functions that do different types of analysis, so we will need to install lots of different packages to do different things. There are over 20,000 different packages currently for R. These are hosted on one of a number of repositories, such as the Comprehensive R Archive Network, or CRAN, which is the official repository for R packages. Other locations where authors store packages include R-Forge and BioconductoR. Many authors host packages in Github repositories, especially for development purposes. Packages are often organized into Task Views, which CRAN uses to organize packages into thematic areas. You can find a list of these Task Views here. There is not a task view for Demography, but there are ones for the Social Sciences, Econometrics, and Spatial Data to name a few. Task views allow users to download a lot of thematically linked packages in a single command, through the package ctv, or Cran Task Views. You can install this package by typing: install.packages(\"ctv\") into Rstudio. Then you have to load the package by using the library() command: library(ctv) which gives you access to the functions in that package. You dont need to install the package again, unless you update your R software, but each time you start a new session (i.e. open Rstudio), you will have to load the library again. If you want to install all of the packages in the Social Science task view, you would type: install.views(\"SocialSciences\") into R and it will install all of the packages under that task view, as of the writing of this sentence, include over 80 packages. I strongly recommend you install several packages prior to us beginning to use R, so you will not be distracted by this later. Ive written a short script on my Github repository and you can use it by running: source(&quot;https://raw.githubusercontent.com/coreysparks/Rcode/master/install_first_short.R&quot;) This will install a few dozen R packages that are commonly used for social science analysis and some other packages I find of use. You only have to install a package once, but each time you start a R session, you must load the package to use its functions. You should also routinely update your installed packages using update.packages(ask = FALSE). This will update any packages that have new versions on CRAN. These often will contain bug fixes and new features. On CRAN, each package will have a README file that tells what has changed in the various versions. Here is one for one of my favorite packages tidycensus. 3.5.1 Functions within packages Each package will have one or more functions within it, each doing a specific task. The default way to access all functions within a given package is to use the command library(packagename) to access the functions. Once loaded, all the functions will be accessible to you. Sometimes, different packages have functions with the same name, for example the base R library has the function lag(), which lags a time series, the dplyr library also has a function lag(), which does a similar task, but with different function arguments. If you have the dplyr library loaded, R will default to use its lag() function. If you want to access a specific function within a specific library, sometimes it is safest to use the library::function() syntax. So if I want to use base Rs lag() function, I could do stats::lag() to access that function specifically. How do you know when this happens? When you load a library, you will often see messages from R that functions have conflicts. For example, if I load dplyr, I see: Conflict messages As you use R more, you will learn which packages have conflicts, and often the developers of the packages will do this and rename the commonly conflicting functions. For example, the function to recode variables in the car package, car::recode() was renamed to car::Recode() to avoid conflicts with the dplyr::recode() function, as both are often used in the same analysis. 3.5.1.1 More notes on functions Functions in R are bits of code that do something, what they do depends on the code within them. For instance, the median() functions underlying code can be seen by: getAnywhere(median.default()) ## A single object matching &#39;median.default&#39; was found ## It was found in the following places ## package:stats ## registered S3 method for median from namespace stats ## namespace:stats ## with value ## ## function (x, na.rm = FALSE, ...) ## { ## if (is.factor(x) || is.data.frame(x)) ## stop(&quot;need numeric data&quot;) ## if (length(names(x))) ## names(x) &lt;- NULL ## if (na.rm) ## x &lt;- x[!is.na(x)] ## else if (any(is.na(x))) ## return(x[FALSE][NA]) ## n &lt;- length(x) ## if (n == 0L) ## return(x[FALSE][NA]) ## half &lt;- (n + 1L)%/%2L ## if (n%%2L == 1L) ## sort(x, partial = half)[half] ## else mean(sort(x, partial = half + 0L:1L)[half + 0L:1L]) ## } ## &lt;bytecode: 0x000000001dfd75e0&gt; ## &lt;environment: namespace:stats&gt; This seems like a lot, I know, but it allows you to see all of the code under the hood of any function. Obviously, the more complicated the function, the more complicated the code. For instance, I can write my own simple function to find the mean of a sample: mymean &lt;- function(x, na.rm = FALSE){ sx &lt;- sum(x, na.rm = FALSE) nx &lt;- length(x) mu &lt;- sx/nx mu } mymean(x = c(1,2,3)) ## [1] 2 This function only includes the basic machinery to calculate the arithmetic mean of a vector \\(x\\). The function has 2 arguments, x and na.rm. All R functions have one or more arguments that users must enter for the function to operate. Some arguments are required, while some are optional, also some arguments, such as the na.rm = FALSE, have default values. As mentioned earlier, to see all the information for a functions arguments, use the help operator, ?. For example ?mean will show you the help documents for the mean() function Mean Function Help When using a new function, its always advised to check out the help file to see all the arguments the function can take, because this is where you can choose alternative specifications for models and methods. These help files also contain the original citations for methods, so you can immediately check the source of the algorithms. The help files also contain a working example of how to use the function on data contained in R. 3.6 Your R user environment When you begin an R session (generally by opening Rstudio) you will begin in your home directory. This is traditionally, on Windows, at 'C:/Users/yourusername/Documents' on Mac at '/Users/yourusername', and on Linux at '/users/yourusername'. There are files you can add to your home directory to specify starting options for R. You can find information on setting up .Rprofile and .Renviron files on CRANs website. This allows you to setup packages that load every time R starts, to save API keys and other various options. These are completely optional and many R users never touch these. If youre not sure where you are you can type getwd(), for get working directory, and R will tell you: getwd() If you dont like where you start, you can change it, by using setwd(), to set your working directory to a new location. setwd(&quot;~&quot;) getwd() R projects will typically set the home folder for the project at the directory location of the project, so files associate with the project will always be in the same place. You can set this at the beginning of your R code file to ensure the code will look for data in a specific location. 3.7 Some Simple R examples Below we will go through a simple R session where we introduce some concepts that are important for R. Im running these in an Rstudio session, in the 3.7.1 R is a calculator #addition and subtraction 3+7 ## [1] 10 3-7 ## [1] -4 #multiplication and division 3*7 ## [1] 21 3/7 ## [1] 0.4285714 #powers 3^2 ## [1] 9 3^3 ## [1] 27 #common math functions log(3/7) ## [1] -0.8472979 exp(3/7) ## [1] 1.535063 sin(3/7) ## [1] 0.4155719 R allows users to write custom functions as well. In general, if you find yourself writing the same code over and over again, you should probably just write a function and save it to your local user environment. For example a very simple function is given below, it takes a variable \\(x\\) as an argument, and then exponentiates the value of the variable. #custom functions myfun &lt;- function(x){ exp(x) } myfun(.5) ## [1] 1.648721 myfun(-.1) ## [1] 0.9048374 You may want to save this function for future use, so you dont have to write it over again. In general, this is why people write R packages, to store custom functions, but you can also save the function to an R script. One such way to do this is to use the dump() command. dump(&quot;myfun&quot;, file=&quot;myfun1.R&quot;) One way to load this function when you want to use it is to use the source() command, which loads any code in a given R script. source(&quot;myfun1.R&quot;) Which will load this function into your local environment and you can use it. If you are interested in writing your own packages, I would highly recommend reading Wickham (n.d.). 3.8 Variables and objects In R we assign values to objects (object-oriented programming). These can generally have any name, but some names are reserved for R. For instance you probably wouldnt want to call something mean because theres a mean() function already in R. For instance: x &lt;- 3 y &lt;- 7 x+y ## [1] 10 x*y ## [1] 21 log(x*y) ## [1] 3.044522 The [1] in the answer refers to the first element of a vector, which brings us to 3.8.1 Vectors R thinks many objects are like a matrix, or a vector, meaning a row or column that contains either numbers or characters. One of Rs big selling points is that much of it is completely vectorized. Meaning, I can apply an operation along all elements of a vector without having to write a loop. For example, if I want to multiply a vector of numbers by a constant, in SAS, I could do: for (i in 1 to 5) x[i] &lt;- y[i]*5 end; but in R, I can just do: x &lt;- c(3, 4, 5, 6, 7) #c() makes a vector y &lt;- 7 x*y ## [1] 21 28 35 42 49 R is also very good about using vectors, lets say I wanted to find the third element of x: x[3] ## [1] 5 or if I want to test if this element is 10 x[3] == 10 ## [1] FALSE x[3] != 10 ## [1] TRUE or is it larger than another number: x[3] &gt; 3 ## [1] TRUE or is any element of the whole vector greater than 3 x &gt; 3 ## [1] FALSE TRUE TRUE TRUE TRUE If you want to see whats in an object, use str(), for structure str(x) ## num [1:5] 3 4 5 6 7 and we see that x is numeric, and has the values that we made. We can also see different characteristics of x #how long is x? length(x) ## [1] 5 #is x numeric? is.numeric(x) ## [1] TRUE #is x full of characters? is.character(x) ## [1] FALSE #is any element of x missing? is.na(x) ## [1] FALSE FALSE FALSE FALSE FALSE #now i&#39;ll modify x x &lt;- c(x, NA) #combine x and a missing value ==NA x ## [1] 3 4 5 6 7 NA #Now ask if any x&#39;s are missing is.na(x) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE 3.8.1.1 Replacing elements of vectors Above, we had a missing value in X, lets say we want to replace it with another value. He we will use basic conditional logic, which exists in any programming language. The ifelse() function will evaluate a test statement, and depending on if that statement is true, it will assign a value, if the statement is false, R will assign another value. Here, we replace the missing value with \\(\\sqrt{7.2}\\), and leave the other values as they are. x &lt;- ifelse(test = is.na(x) == TRUE, yes = sqrt(7.2), no = x) x ## [1] 3.000000 4.000000 5.000000 6.000000 7.000000 2.683282 3.9 Variable types R stores data differently depending on the type of information contained. Common variables types in R are numeric, character, integer and factor. Numeric variables are just that, numbers. They can be whole numbers or decimal values. The best way to see if a variable is numeric is to use is.numeric(x), and R will return TRUE if the variable is numeric and FALSE if it is not. is.numeric(x) ## [1] TRUE Likewise, you can use is.character(), is.integer(), and is.factor to identify if a variable is of a given type. The class() function will also do this more generally: class(x) ## [1] &quot;numeric&quot; Character and factor variables often store the same kind of information, and R (until recently) would always convert character variables to factors when data were read into R. This is the option getOption(\"stringsAsFactors\"), which used to default to True, but has recently changed. Whats the difference you ask? Character variables store information on strings, or text. This is one way to code categorical variables that are strings. Factors, on the other hand can store strings OR numbers as categorical variables, and can be ordered or unordered. Factors also allow for specific categories of the variable to be considered as reference categories, as are often used in many statistical procedures. Factor variables have levels which are the different values of the categorical variable, this implied a more complicated structure than simple character variables, which lack these qualities. You can manipulate variables of one type into another, with some notable things to watch out for. Here are some examples: #create at numeric vector z &lt;- c(1,2,3,4) class(z) ## [1] &quot;numeric&quot; We can convert this to a character vector using as.character() zc &lt;- as.character(z) zc ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; Likewise, we can convert it to a factor type: zf &lt;- as.factor(z) zf ## [1] 1 2 3 4 ## Levels: 1 2 3 4 class(zf) ## [1] &quot;factor&quot; is.ordered(zf) ## [1] FALSE and as an ordered factor: zfo &lt;- factor(zf, ordered = TRUE) zfo ## [1] 1 2 3 4 ## Levels: 1 &lt; 2 &lt; 3 &lt; 4 Another very useful variable type is the logical type. In R a logical variable is either a TRUE or FALSE value. I personally use this a lot in my work in both preliminary data analysis and data checking. We saw this used above, when we did is.na(x) == TRUE to check if the x variable was missing. We can see how this translates into a logical variable here: x &lt;- c(3, 4, 5, 6, 7, NA) z&lt;-is.na(x) #check if x is missing z ## [1] FALSE FALSE FALSE FALSE FALSE TRUE class(z) ## [1] &quot;logical&quot; In practice, I use this with the I() function (more on this below) to do quick binary codes of a value: x &lt;- c(3, 4, 5, 6, 7) table( I(x &gt;= 5) ) ## ## FALSE TRUE ## 2 3 3.10 Dataframes Traditionally, R organizes variables into data frames, these are like a spreadsheet. The columns can have names, and the dataframe itself can have data of different types. Here we make a short data frame with three columns, two numeric and one factor: mydat &lt;- data.frame( x = c(1,2,3,4,5, 6, 7, 8), y = c(10, 20, 35, 57, 37, 21, 23, 25), group = factor(c(&quot;A&quot;, &quot;A&quot; ,&quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;,&quot;C&quot;,&quot;C&quot;)) ) #See the size of the dataframe dim(mydat) ## [1] 8 3 #Open the dataframe in a viewer and just print it print(mydat) ## x y group ## 1 1 10 A ## 2 2 20 A ## 3 3 35 A ## 4 4 57 B ## 5 5 37 B ## 6 6 21 C ## 7 7 23 C ## 8 8 25 C 3.10.1 Accessing variables in dataframes R has a few different ways to get a variable from a data set. One way is the $ notation, used like dataset$variable, and another is to provide the column index or name of the variable. These three methods are illustrated below. The first tells R to get the variable named group from the data. The second tells R to get the column named group from the data, and the third tells R to get the third column from the data. mydat$group ## [1] A A A B B C C C ## Levels: A B C mydat[&#39;group&#39;] ## group ## 1 A ## 2 A ## 3 A ## 4 B ## 5 B ## 6 C ## 7 C ## 8 C mydat[,3] ## [1] A A A B B C C C ## Levels: A B C The names() function is very useful for seeing all the column names of a dataset, without having to print any of the data. names(mydat) ## [1] &quot;x&quot; &quot;y&quot; &quot;group&quot; R has several useful function for previewing the contents of a dataframe or variable. The head() function shows the first 6 observations of a dataframe or variable, and tail() shows the last 6 observations. You can also show a custom number of observations by using the n= argument in either function. These are illustrated below: head(mydat) ## x y group ## 1 1 10 A ## 2 2 20 A ## 3 3 35 A ## 4 4 57 B ## 5 5 37 B ## 6 6 21 C head(mydat, n = 2) ## x y group ## 1 1 10 A ## 2 2 20 A head(mydat$group) ## [1] A A A B B C ## Levels: A B C tail(mydat) ## x y group ## 3 3 35 A ## 4 4 57 B ## 5 5 37 B ## 6 6 21 C ## 7 7 23 C ## 8 8 25 C tail(mydat, n = 2) ## x y group ## 7 7 23 C ## 8 8 25 C tail(mydat$group) ## [1] A B B C C C ## Levels: A B C 3.10.2 Nicer looking tables R can also produce nicely formatted HTML and LaTeX tables. There are several packages that do this, but the knitr package has some basic table creation functions that do a good job for simple tables. library(knitr) kable(mydat, caption = &quot;My basic table&quot;, align = &#39;c&#39;, format = &quot;html&quot;) library(knitr) kable(mydat, caption = &quot;My basic table&quot;, align = &#39;c&#39;, format=&quot;latex&quot; ) Much more advanced tables can be created using the gt package Iannone, Cheng, and Schloerke (2020), which allows for highly customized tables. library(gt, quietly = TRUE) library(dplyr, quietly = TRUE) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union mydat%&gt;% gt()%&gt;% tab_header(title= &quot;My simple gt table&quot;, subtitle = &quot;With a subtitle&quot;) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #rwiwxqpihf .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #rwiwxqpihf .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rwiwxqpihf .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #rwiwxqpihf .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #rwiwxqpihf .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rwiwxqpihf .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rwiwxqpihf .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #rwiwxqpihf .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #rwiwxqpihf .gt_column_spanner_outer:first-child { padding-left: 0; } #rwiwxqpihf .gt_column_spanner_outer:last-child { padding-right: 0; } #rwiwxqpihf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #rwiwxqpihf .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #rwiwxqpihf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #rwiwxqpihf .gt_from_md > :first-child { margin-top: 0; } #rwiwxqpihf .gt_from_md > :last-child { margin-bottom: 0; } #rwiwxqpihf .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #rwiwxqpihf .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #rwiwxqpihf .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rwiwxqpihf .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #rwiwxqpihf .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rwiwxqpihf .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #rwiwxqpihf .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #rwiwxqpihf .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rwiwxqpihf .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rwiwxqpihf .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #rwiwxqpihf .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rwiwxqpihf .gt_sourcenote { font-size: 90%; padding: 4px; } #rwiwxqpihf .gt_left { text-align: left; } #rwiwxqpihf .gt_center { text-align: center; } #rwiwxqpihf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #rwiwxqpihf .gt_font_normal { font-weight: normal; } #rwiwxqpihf .gt_font_bold { font-weight: bold; } #rwiwxqpihf .gt_font_italic { font-style: italic; } #rwiwxqpihf .gt_super { font-size: 65%; } #rwiwxqpihf .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } My simple gt table With a subtitle x y group 1 10 A 2 20 A 3 35 A 4 57 B 5 37 B 6 21 C 7 23 C 8 25 C 3.11 Real data example Now lets open a real data file. This is the 2018 World population data sheet from the Population Reference Bureau. It contains summary information on many demographic and population level characteristics of nations around the world in 2018. Ive had this entered into a Comma Separated Values file by some poor previous research assistant of mine and it lives happily on Github now for all the world to see. CSV files are a good way to store data coming out of a spreadsheet, because R can read them without any other packages. R can also read Excel files, but it requires external packages to do so, such as readxl. I can read it from Github directly by using a function in the readr library, or with the base R function read.csv(), both accomplish the same task. prb &lt;- read.csv(file = &quot;https://raw.githubusercontent.com/coreysparks/r_courses/master/2018_WPDS_Data_Table_FINAL.csv&quot;, stringsAsFactors = TRUE) Thats handy. If the file lived on our computer in your working directory, I could read it in like so: prb &lt;- read_csv(&quot;path/to/file/2018_WPDS_Data_Table_FINAL.csv&quot;) Same result. The haven library Wickham and Miller (2020) can read files from other statistical packages easily, so if you have data in Stata, SAS or SPSS, you can read it into R using those functions, for example, the read_dta() function reads Stata files, read_sav() to read SPSS data files. library(haven) prb_stata &lt;- read_dta(&quot;path/to/file/prb2018.dta&quot;) prb_spss &lt;- read_sav(&quot;path/to/file/prb_2018.sav&quot;) 3.12 Basic Descriptive analysis of data One of the key elements of analyzing data is the initial descriptive analysis of it. In subsequent chapters, I will go into more depth about this process, but for now, I want to illustrate some simple but effective commands for summarizing data. 3.12.1 Dataframe summaries The summary() function is very useful both in terms of producing numerical summaries of individual variables, but also for shows summaries of entire dataframes. Its output differs based on the type of variable you give it, for character variables it does not return any summary. For factor variables, it returns a frequency table, and for numeric variables, it returns the five number summary plus the mean. summary(prb$region) ## CARIBBEAN CENTRAL AMERICA CENTRAL ASIA EAST ASIA ## 17 8 5 8 ## EASTERN AFRICA EASTERN EUROPE MIDDLE AFRICA NORTHERN AFRICA ## 20 10 9 7 ## NORTHERN AMERICA NORTHERN EUROPE OCEANIA SOUTH AMERICA ## 2 11 17 13 ## SOUTH ASIA SOUTHEAST ASIA SOUTHERN AFRICA SOUTHERN EUROPE ## 9 11 5 15 ## WESTERN AFRICA WESTERN ASIA WESTERN EUROPE ## 16 18 9 summary(as.factor(prb$continent)) ## AFRICA ASIA EUROPE NORTHERN AMERICA ## 57 51 45 27 ## OCEANIA SOUTH AMERICA ## 17 13 summary(prb$tfr) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.600 2.300 2.709 3.750 7.200 I find this function to be very useful when Im initially exploring a data set, so I can easily see the min/max values of a variable. There are many alternatives to this base function, including psych::describe(), Hmisc::describe(), and skimr::skim(), all of which produce summaries of dataframes or variables desc1 &lt;- psych::describe(prb[, 1:8], fast = FALSE) print(desc1, short = TRUE) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 8 rows ] desc2 &lt;- Hmisc::describe(prb[, 1:8], tabular= FALSE) head(desc2) ## prb[, 1:8] ## ## 6 Variables 210 Observations ## -------------------------------------------------------------------------------- ## continent ## n missing distinct ## 210 0 6 ## ## lowest : AFRICA ASIA EUROPE NORTHERN AMERICA OCEANIA ## highest: ASIA EUROPE NORTHERN AMERICA OCEANIA SOUTH AMERICA ## ## Value AFRICA ASIA EUROPE NORTHERN AMERICA ## Frequency 57 51 45 27 ## Proportion 0.271 0.243 0.214 0.129 ## ## Value OCEANIA SOUTH AMERICA ## Frequency 17 13 ## Proportion 0.081 0.062 ## -------------------------------------------------------------------------------- ## region ## n missing distinct ## 210 0 19 ## ## lowest : CARIBBEAN CENTRAL AMERICA CENTRAL ASIA EAST ASIA EASTERN AFRICA ## highest: SOUTHERN AFRICA SOUTHERN EUROPE WESTERN AFRICA WESTERN ASIA WESTERN EUROPE ## -------------------------------------------------------------------------------- ## country ## n missing distinct ## 210 0 210 ## ## lowest : Afghanistan Albania Algeria Andorra Angola ## highest: Vietnam Western Sahara Yemen Zambia Zimbabwe ## -------------------------------------------------------------------------------- ## pop2018 ## n missing distinct Info Mean Gmd .05 .10 ## 210 0 142 1 36.29 59.09 0.10 0.20 ## .25 .50 .75 .90 .95 ## 1.30 7.00 25.43 65.21 117.95 ## ## lowest : 0.0 0.1 0.2 0.3 0.4, highest: 209.4 265.2 328.0 1371.3 1393.8 ## -------------------------------------------------------------------------------- ## cbr ## n missing distinct Info Mean Gmd .05 .10 ## 210 0 37 0.998 20.34 11.47 9.0 9.0 ## .25 .50 .75 .90 .95 ## 11.0 18.5 28.0 35.1 39.0 ## ## lowest : 7 8 9 10 11, highest: 40 41 43 45 48 ## -------------------------------------------------------------------------------- ## cdr ## n missing distinct Info Mean Gmd .05 .10 ## 210 0 16 0.986 7.676 3.061 4 5 ## .25 .50 .75 .90 .95 ## 6 7 9 11 13 ## ## lowest : 1 2 3 4 5, highest: 12 13 14 15 16 ## ## Value 1 2 3 4 5 6 7 8 9 10 11 ## Frequency 1 4 5 8 25 37 29 26 25 18 14 ## Proportion 0.005 0.019 0.024 0.038 0.119 0.176 0.138 0.124 0.119 0.086 0.067 ## ## Value 12 13 14 15 16 ## Frequency 5 6 5 1 1 ## Proportion 0.024 0.029 0.024 0.005 0.005 ## -------------------------------------------------------------------------------- desc3 &lt;- skimr::skim(prb[, 1:8]) desc3 The skimr::skim() function is very good at doing summaries of both numeric and categorical data, while the other functions are perhaps best suited to numeric data. The summary() function, as well as the other three functions in other packages can be used on a single variable within a dataframe as well, or on a simple vector: summary(prb$tfr) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.600 2.300 2.709 3.750 7.200 summary(zf) ## 1 2 3 4 ## 1 1 1 1 From this summary, we see that the mean is 2.7085714, there is one country missing the Total fertility rate variable. The minimum is 1 and the maximum is 7.2 children per woman. 3.12.2 Frequency tables A basic exploration of data, especially if your data have categorical or nominal variables, includes the extensive use of frequency tables. If youre simply looking at the number of observations in each level of a categorical variable, or using frequency tables to aggregate data, they are some of the most useful basic statistical summaries around. The basic function for constructing simple tables is table() in base R. More sophisticated table construction is allowed in xtabs() Lets have a look at some descriptive information about the data: #Frequency Table of # of Countries by Continent table(prb$continent) ## ## AFRICA ASIA EUROPE NORTHERN AMERICA ## 57 51 45 27 ## OCEANIA SOUTH AMERICA ## 17 13 Frequency of TFR over 3 by continent: table(I(prb$tfr &gt; 3), prb$continent) ## ## AFRICA ASIA EUROPE NORTHERN AMERICA OCEANIA SOUTH AMERICA ## FALSE 11 40 45 27 7 12 ## TRUE 46 11 0 0 10 1 Two things to notice in the above code, first we have to use the $ operator to extract each variable from the prb dataframe. Second, the I() operator is used. This is honestly one of my favorite things in base R. I() is the indicator function, it evaluates to TRUE or FALSE depending on the argument inside of it. This also allows for fast construction of binary variables on the fly in any function. Heres another example: x &lt;- c(1, 3, 4, 5, 7, 19) I(x &gt; 5) ## [1] FALSE FALSE FALSE FALSE TRUE TRUE table(I(x &gt; 5)) ## ## FALSE TRUE ## 4 2 So we see how this works, I checks if x is greater than 5, if it is, I() returns TRUE. When we feed this to table(), we can count up the TRUE and FALSE responses. Later in the book, we will see how to employ the xtabs() function to quickly aggregate data from individual level to aggregate level. 3.12.3 More basic statistical summaries Now, we will cover some basic descriptive statistical analysis including basic measures of central tendency and variability. 3.12.4 Measures of central tendency We can use graphical methods to describe what data look like in a visual sense, but graphical methods are rarely useful for comparative purposes. In order to make comparisons, you need to rely on a numerical summary of data vs. a graphical one. Numerical measures tell us a lot about the form of a distribution without resorting to graphical methods. The first kind of summary statistics we will see are those related to the measure of central tendency. Measures of central tendency tell us about the central part of the distribution 3.12.5 Mean and median Here is an example from the PRB data. mean(prb$tfr) ## [1] 2.708571 Whoops! What happened? This means that R cant calculate the mean because theres a missing value, which we saw before. We can tell R to automatically remove missing values by: mean(prb$tfr, na.rm = TRUE) ## [1] 2.708571 Which works without an error. Many R functions will fail, or do listwise deletion of observations when NAs are present, so its best to look at the documentation for the function youre wanting to use to see what its default na action is. The mean() function defaults to na.rm = FALSE, which indicates that it does not remove missing values by default. We can also calculate the median TFR median(prb$tfr, na.rm = TRUE) ## [1] 2.3 3.12.6 Measures of variation One typical set of descriptive statistics that is very frequently used is the so-called five number summary and it consists of : the Minimum, lower quartile, median, upper quartile and maximum values. This is often useful if the data are not symmetric or skewed. This is what you get when you use the fivenum() function, or we can include the mean if we use the summary() function. fivenum(prb$tfr) ## [1] 1.0 1.6 2.3 3.8 7.2 summary(prb$tfr) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.600 2.300 2.709 3.750 7.200 3.12.6.1 Variance To calculate the variance and standard deviation of a variable: var(prb$tfr, na.rm = TRUE) #variance ## [1] 1.806338 sd(prb$tfr, na.rm = TRUE) #standard deviation ## [1] 1.344001 sqrt(var(prb$tfr)) #same as using sd() ## [1] 1.344001 The above sections have shown some basic ways to summarize data in R, along with many handy functions that are pervasive in my own general work flow. Is this everything R will do, No. Are these the only way to do things in R? Never. Im constantly marveled at how many new functions I see my students using in their own work and this reminds me how much of the R ecosystem I have yet to explore, even after twenty-plus years of using it. 3.13 The tidyverse So far, most of the functions I have discussed have been from the base R ecosystem, with some specific functions from other downloadable packages. One of the biggest changes to R in recent years has been the explosion in popularity of the tidyverse Wickham et al. (2019). The tidyverse is a large collection of related packages that share a common philosophy of how data and programming relate to one another and work together to produce a more streamlined, literate way of programming with data. To get the core parts of the tidyverse, install it using install.packages(\"tidyverse\") in your R session. This will install the core components of the tidyverse that can then be used throughout the rest of the book.1 Two of the workhorses in the tidyverse are the packages dplyr Wickham et al. (2020) and ggplot2 Wickham (2016). The dplyr package is very thoroughly described in the book R for Data Science Wickham and Grolemund (2017), and the ggplot2 package also has a book-length description in the book ggplot2: Elegant Graphics for Data Analysis Wickham (2016), so I wont waste time and space here with complete descriptions. Instead, I will show some pragmatic examples of how these work in my own work flow, and also use these packages together to produce some descriptive data visualizations. 3.13.1 Basic dplyr The dplyr package has many functions that work together to produce succinct, readable and highly functional code. I often say about base R packages in comparison to things like SAS, that I can do something in R in about 10 lines of code compared to 50 in SAS. Using dplyr, you can do even more, faster. The package consists of core verbs that are used to clean, reshape, and summarize data. Using pipes, the user can chain these verbs together so that you only have to name the data being used once, which makes for more efficient code, since youre not constantly having to name the dataframe. The pipes also allow for all variables within a dataframe to be accessed, without using the $ or [] notation described earlier in this chapter. Perhaps a short tour of using dplyr would be good at this point, and we will see it used throughout the book. In the following code, I will use the prb data from earlier, and I will do a series of tasks. First, I will create a new variable using the mutate() function, then group the data into groups (similar to SASs by processing) , then do some statistical summaries of other variables using the summarise() function. Here we go: library(dplyr) prb %&gt;% mutate(high_tfr = ifelse(test = tfr &gt; 3, yes = &quot;high&quot;, no = &quot;low&quot;) )%&gt;% group_by(high_tfr) %&gt;% summarise(mean_e0 = mean(e0male, na.rm = TRUE)) ## # A tibble: 2 x 2 ## high_tfr mean_e0 ## &lt;chr&gt; &lt;dbl&gt; ## 1 high 62.8 ## 2 low 73.6 The prb%&gt;% line says, take the prb data and feed it into the next verb using the pipe. The next line mutate(high_tfr = ifelse(test = tfr &gt; 3,yes = \"high\", no = \"low\") )%&gt;% tells R to create a new variable called high_tfr, the value of the variable will be created based on conditional logic. If the value of the tfr is over 3, the value will be \"high\" and if the value of the tfr is less than 3, the value of the variable will be \"low\". The group_by(high_tfr)%&gt;% line tells R to form a grouped data frame, basically this is how dplyr segments data into discrete groups, based off a variable, and then performs operations on those groups. This is the same thing as stratification of data. The final command summarise(mean_e0 = mean(e0male, na.rm = TRUE)) tells R to take the mean of the e0male variable, in this case it will be calculated for each of the high_tfr groups. Finally, we ungroup() the dataframe to remove the grouping, this is customary whenever using the group_by() verb. We can also summarize multiple variables at the same time using the across() command. In the code below, I find the mean (specified by .fns = mean) for each of the four variables e0male, e0female, gnigdp and imr for each of the high_tfr groups. prb %&gt;% mutate(high_tfr = ifelse(test = tfr &gt; 3, yes = &quot;high&quot;, no = &quot;low&quot;) )%&gt;% group_by(high_tfr) %&gt;% summarise(n = n(), across(.cols = c(e0male, e0female, gnigdp, imr), .fns = mean, na.rm = TRUE))%&gt;% ungroup() ## # A tibble: 2 x 6 ## high_tfr n e0male e0female gnigdp imr ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 high 68 62.8 66.4 5329. 43.2 ## 2 low 142 73.6 78.9 27216. 11.9 The line summarise(n=n() , across(.cols = c(e0male, e0female, gnigdp, imr), .fns = mean, na.rm = TRUE)) tells R to first count the number of cases in each group n = n(), then summarize multiple variables, in this case male and female life expectancy at birth, GDP, and the infant mortality rate, by each of the levels of the high_tfr variable. The summary I want to do is the mean of each variable, being sure to remove missing values before calculating the mean. We see then the estimates of the four other indicators for countries that have TFR over 3, versus countries with a TFR under 3. This is a basic dplyr use, but it is far from what the package can do. Throughout the rest of the book, this process will be used to do calculations, aggregate data, present model results and produce graphics. This example was trying to show a simple workflow in dplyr, and introduce the pipe concept. Next, we will explore some basic uses of dplyr in conjunction with the ggplot2 package. 3.14 Basic ggplot Lets say that we want to compare the distributions of income from the above examples graphically. Since the ggplot2 library is part of the tidyverse, it integrates directly with dplyr and we can do plots within pipes too. In generally, ggplot() has a few core statements. ggplot() statement - This tells R the data and the basic aesthetic that will be plotted, think x and y axis of a graph. The aesthetic is defined using the aes() function. This is where you pass values to be plotted to the plot device. Define the geometries you want to use to plot your data, there are many types of plots you can do, some are more appropriate for certain types of data Plot annotations - Titles, labels etc. This allows you to customize the plot with more information to make it more easily understandable. Now I will illustrate some basic ggplot examples, and Im going to use the PRB data that I have been using for other examples. In order to better illustrate the code, I will walk through a very minimal example, line by line. library(ggplot2) Loads the ggplot package ggplot(data = prb, mapping = aes(x = tfr))+ Use the ggplot function, on the prb dataframe. The variable we are plotting is the total fertility rate, tfr. In this case, it is the only variable we are using. I include a + at the end of the line to tell R that more elements of the plot are going to be added. geom_histogram()+ Tells R that the geometry we are using is a histogram, again we have the + at the end of the line to indicate that we will add something else to the plot, in this case a title. ggtitle(label = \"Distribution of the Total Fertility Rate, 2018\") Tells R the primary title for the plot, which describes what is being plotted. Im also going to add an additional annotation to the x-axis to indicate that it is showing the distribution of the TFR: xlab(label = \"TFR\") Now, lets see all of this together: library(ggplot2) ggplot(data=prb, mapping=aes(x = tfr))+ geom_histogram()+ ggtitle(label = &quot;Distribution of the Total Fertility Rate, 2018&quot;)+ xlab(label = &quot;TFR&quot;) The above example named the data frame explicitly in the ggplot() call, but we can also use dplyr to pipe data into the plot: prb%&gt;% ggplot(mapping=aes(x = tfr))+ geom_histogram()+ ggtitle(label = &quot;Distribution of the Total Fertility Rate, 2018&quot;)+ xlab(label = &quot;TFR&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can likewise incorporate a dplyr workflow directly into our plotting, using the example from before, we will create histograms for the high and low fertility groups using the facet_wrap() function. prb%&gt;% mutate(high_tfr = ifelse(test = tfr &gt; 3, yes = &quot;high&quot;, no = &quot;low&quot;) )%&gt;% group_by(high_tfr)%&gt;% ggplot(mapping=aes(x = imr))+ geom_histogram(aes( fill = high_tfr))+ facet_wrap( ~ high_tfr)+ ggtitle(label = &quot;Distribution of the Infant Mortality Rate, 2018&quot;, subtitle = &quot;Low and High Fertility Countries&quot;)+ xlab(label = &quot;Infant Mortality Rate&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1 rows containing non-finite values (stat_bin). You also notice that I used the aes(fill = high_tfr) to tell R to color the histogram bars according to the variable high_tfr. The aes() function allows you to modify colors, line types, and fills based of values of a variable. Another way to display the distribution of a variable is to use geom_density() which calculates the kernel density of a variable. Again, I use a variable, this time the continent a country is on, to color the lines for the plot. prb%&gt;% ggplot(mapping = aes(tfr, colour = continent, stat = ..density..))+ geom_density()+ ggtitle(label = &quot;Distribution of the Total Fertility Rate by Continent&quot;, subtitle = &quot;2018 Estimates&quot;)+ xlab(label = &quot;TFR&quot;) 3.14.1 Stem and leaf plots/Box and Whisker plots Another visualization method is the stem and leaf plot, or box and whisker plot. This is useful when you have a continuous variable you want to display the distribution of across levels of a categorical variable. This is basically a graphical display of Tukeys 5 number summary of data. prb%&gt;% ggplot( mapping = aes(x = continent, y = tfr))+ geom_boxplot()+ ggtitle(label = &quot;Distribution of the Total Fertility Rate by Continent&quot;, subtitle = &quot;2018 Estimates&quot;) You can flip the axes, by adding coord_flip() prb%&gt;% ggplot( mapping = aes( x = continent, y = tfr))+ geom_boxplot()+ ggtitle(label = &quot;Distribution of the Total Fertility Rate by Continent&quot;, subtitle = &quot;2018 Estimates&quot;)+ coord_flip() You can also color the boxes by a variable, Here, I will make a new variable that is the combination of the continent variable with the region variable, using the paste() function. Its useful for combining values of two strings. prb%&gt;% mutate(newname = paste(continent, region, sep = &quot;-&quot;))%&gt;% ggplot(aes(x = newname, y = tfr, fill = continent))+ geom_boxplot()+ coord_flip()+ ggtitle(label = &quot;Distribution of the Total Fertility Rate by Continent&quot;, subtitle = &quot;2018 Estimates&quot;) 3.14.2 X-Y Scatter plots These are useful for finding relationships among two or more continuous variables. ggplot() can really make these pretty. The geom_point() geometry adds points to the plot. Here are a few riffs using the PRB data: prb%&gt;% ggplot(mapping= aes(x = tfr, y = imr))+ geom_point()+ ggtitle(label = &quot;Relationship between Total Fertility and Infant Mortality&quot;, subtitle = &quot;2018 Estimates&quot;)+ xlab(label = &quot;TFR&quot;)+ ylab(label = &quot;IMR&quot;) ## Warning: Removed 1 rows containing missing values (geom_point). R also makes it easy to overlay linear and spline smoothers for the data (more on splines later). prb%&gt;% ggplot(mapping = aes(x = tfr, y = imr))+ geom_point()+ geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;, se = F)+ #linear regression fit geom_smooth(color = &quot;blue&quot;, method = &quot;loess&quot;, se = FALSE)+ ggtitle(label = &quot;Relationship between Total Fertility and Infant Mortality&quot;, subtitle = &quot;2018 Estimates&quot;)+ xlab(label = &quot;TFR&quot;)+ ylab(label = &quot;IMR&quot;) Now we color the points by continent prb%&gt;% ggplot(mapping = aes(x = tfr, y = imr, color =continent))+ geom_point()+ geom_smooth(method = &quot;lm&quot;, se = FALSE)+ ggtitle(label = &quot;Relationship between Total Fertility and Infant Mortality&quot;, subtitle = &quot;2018 Estimates&quot;)+ xlab(label = &quot;TFR&quot;)+ ylab(label = &quot;IMR&quot;) 3.14.3 Facet plots Facet plots are nice, they allow you to create a plot separately based on a grouping variable. This allows you to visualize if the relationship is constant across those groups. Here, I repeat the plot above, but I facet on the continent, and include the regression line for each continent. prb%&gt;% ggplot(mapping= aes(x = tfr, y = imr, color = continent))+ geom_point()+ geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;)+ facet_wrap( ~ continent)+ ggtitle(label = &quot;Relationship between Total Fertility and Infant Mortality&quot;, subtitle = &quot;2018 Estimates&quot;)+ xlab(label = &quot;TFR&quot;)+ ylab(label = &quot;IMR&quot;) Another example, this time of a bad linear plot! ggplot makes it easy to examine if a relationship is linear or curvilinear, at least visually. ggplot(data = prb,mapping = aes(x = tfr, y = pctlt15_2018))+ geom_point()+ geom_smooth( method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;)+ geom_smooth( method = &quot;loess&quot;, se = FALSE, color = &quot;blue&quot;)+ ggtitle(label = &quot;Relationship between Total Fertility and Percent under age 15&quot;, subtitle = &quot;2018 Estimates- Linear &amp; Loess fit&quot;)+ xlab(label = &quot;Percent under age 15&quot;)+ ylab(label = &quot;IMR&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.15 Chapter summary In this chapter, I have introduced R and Rstudio and some basic uses of the software for accessing data and estimating some summary statistics. The R ecosystem is large and complex, and the goal of this book is to show you, the user, how to use R for analyzing data from demographic data sources. In the chapters that follow, I will show how to use R within two large universes of data, the macro and the micro. The macro level sections will focus on using R on data that come primarily from places - nations, regions, administrative areas. The micro level sections will focus on analyzing complex survey data on individual responses to demographic surveys. The final section will discuss approaches that merge these two levels into a multi-level framework and describe how such models are estimated and applied. 3.16 References References "],["survey-data-analysis.html", "Chapter 4 Survey data analysis 4.1 Demographic Survey data 4.2 Basics of survey sampling 4.3 Simple versus complex survey designs 4.4 Characteristics of YOUR survey 4.5 Example from the American Community Survey 4.6 Basics of analyzing survey data 4.7 Replicates and jack knifes and expansions, oh my! 4.8 Descriptive analysis of survey data 4.9 Weighted frequencies and rates 4.10 Creating tables from survey data analysis 4.11 Basic statistical testing on survey data. 4.12 More examples from the CDC BRFSS 4.13 Regression example 4.14 Creating Survey estimates for places 4.15 Replicate Weights", " Chapter 4 Survey data analysis 4.1 Demographic Survey data The majority of demographic research relies on two or three main sources of information. First among these are population enumerations or censuses, followed by vital registration data on births and deaths and last but not least, data from surveys. Censuses and other population enumerations are typically undertaken by federal statistical agencies and demographers use this data once its disseminated from these agencies. Similarly, vital registration data are usually collected by governmental agencies, who oversee the collection and data quality for the data. Survey data on the other hand can come from a wide variety of sources. Its not uncommon for us to go and collect our own survey data specific to a research project we have, typically on a specialized population that we are interested in learning about, but surveys can also be quite general in their scope and collect information on a wide variety of subjects. Owing to the mix of small and large-scale survey data collection efforts, survey data are often available on many different topics, locales and time periods. Of course we as demographers are typically interested in population-level analysis or generalization from our work, so the survey data we try to use are collected in rigorous manners, with much attention and forethought paid to ensure the data we collect can actually be representative of the target population we are trying to describe. In this chapter, I will introduce the nature of survey sampling as is often used in demographic data sources, and describe what to look for when first using a survey data source for you research. These topics are geared towards researchers and students who have not worked with survey data much in the past and will go over some very pragmatic things to keep in mind. Following this discussion, I will use a specific example from the US Census Bureaus American Community Survey and illustrate how to apply these principals to this specific source. The final goal of this chapter is to show how to use R to analyze survey data and produce useful summaries from our surveys, both tabular and graphically. 4.2 Basics of survey sampling To begin this section, I want to go over some of the simple terms from sampling that are very important to those of us who rely on survey data for our work. For many of the concepts from this chapter, I strongly recommend Lohr (2019) for the theoretical portions and Lumley (2010) for discussion of how R is used for complex survey data. The target population is the population that our survey has been designed to measure. For large national surveys, these are typically the population of the country of interest. For example, the Demographic and Health Survey (DHS) has its primary target population as women of childbearing ages in women of reproductive age and their young children living in households. Our observational units are the level at which we are collecting data, for surveys this is typically a person or a household, and our survey documentation will tell us what its unit of observation is. Sampling Units refer to the units that can serve for us to collect data from, for example we may not have a list of every school age child, but we may have a list of schools, so we may use schools as our sampling units and sample children within them. The sampling frame is the set of sampling units containing distinct sets of population members, this is usually the most recent population census, ideally the entire population, or following our school example from above, the entire listing of schools. These terms are ubiquitous in sampling, but other terminology also exists in many surveys and these terms relate to the nature of how the survey was actually carried out. Many times the surveys we end up using are not themselves simple random samples, but are instead some blend of stratified or cluster sample. For example, the DHS uses a stratified, cluster sample to collect its information. Strata refer to relatively homogeneous areas within the place we are trying to collect data. In the DHS, these are typically rural or urban areas of a country, as identified by the census. Within each strata, the DHS will choose clusters from which to sample from, this is a two-stage sampling method, where first the sampling frame is stratified, then clusters are selected. Clusters in the DHS are usually neighborhoods in urban areas and smaller towns or villages in rural areas. Figure 1 shows a cartoon of how this process works, with multiple potential cluster that can be sampled (boxes), and within the cluster are our observational units, some of which are sampled, and some of which are unsampled. Figure 1 4.3 Simple versus complex survey designs How the data were using is sampled has a major implication for how we analyze it. The majority of statistical tools assume that data come from simple random samples, because most methods assume independence of observations, regardless of which distribution or test statistic you are using. Violations of this assumption are a big problem when we go to analyze our data, because the non-independence of survey data are automatically in violation of a key assumption of any test. The stratified and clustered nature of many survey samples may also present problems for methods such as linear regression analysis which assume errors in the model are homoskedastic, or constant. When data are collected in a stratified or clustered method, the data may have less variation than a simple random sample, because individuals who live closely to one another often share other characteristics in common as well. Our statistical models dont do well with this type of reduction in variation and we often have to resort to manipulations of our model parameters or standard errors of our statistics in order to make them coincide with how the data were collected. Not to fear! Data collected using public funds are typically required to be made available to the public with information on how to use them. Most surveys come with some kind of code book or user manual which describes how the data were collected and how you should go about using them. In these cases, it pays to read the manual because it will tell you the names of the stratification and clustering variables in the survey data. This will allow you to use the design of the survey in your analysis so that your statistical routines are corrected for the non-randomness and homogeneity in the survey data. Hes not heavy, hes my brother Another important aspect of survey data are the use of weighting variables. Whenever we design a survey, we have our target population, or universe of respondents in mind. In the DHS, again, this is traditionally2 women of childbearing age and their children (International 2012). When we collect a sample from this population, or sample may be, and typically is, imperfect. It is imperfect for many reasons, owing to the difficulty of sampling some members of the population, or their unwillingness to participate in our study. Part of designing an effective survey is knowing your universe or population, and its characteristics. This will let you know the probability of a particular person being in the sample. Of course, the more complicated the survey, the more complicated it is to know what this probability is. For example, if we were to sample people in the United States, using a stratified design based on rural and urban residence, we would need to know how many people lived in rural and urban areas within the country, as this would effect the probability of sampling a person in each type of area. This inclusion probability tells us how likely a given person is of being sampled. The inverse of the inclusion probability is called the sampling weight: \\(w_i = \\frac{1} {\\pi_i}\\) where \\(\\pi_i\\) is the inclusion probability. Sampling weights are what we use to make our analyses of a survey representative of the larger population. They serve many purposes including unequal inclusion probabilities, differences in sample characteristics compared to the larger population, and differences in response rates across sample subgroups. All of these situations make the sample deviate from the population by affecting who the actual respondents included in the survey are. Differences in our sample when compared to the larger population can affect most all of our statistical analysis since again, most methods assume random sampling. The weights that are included in public data are the result of a rigorous process conducted by those who designed and implemented the survey itself, and most surveys in their user manuals or code books describe the process of how the weights are created. For example, the US Center for Disease Control and Preventions Behavioral Risk Factor Surveillance System (BRFSS) provides a very thorough description of how their final person weights are calculated (CDC 2020). These weights include three primary factors, the stratum weight, which is a combination of the number of records in a sample strata and the density of phone lines in a given strata, combined with the number of phones in a sampled household and the number of adults in the household to produce the final design weight. These weights are then raked to eight different marginal totals, based on age, race/ethnicity, education, marital status, home ownership, gender by race/ethnicity, age by race/ethnicity and phone ownership(CDC 2020). After this process, weights are interpretable as the number of people a given respondent in the survey represents in the population. So, if a respondents weight in the survey data is 100, they actually represent 100 people in the target population. Other types of weights also exist, and are commonly seen in federal data sources. A common kind of weight that includes information on both the probability of inclusion AND the stratified design of the survey are replicate weights. Replicate weights are multiple weights for each respondent, and there are as many weights as there are different levels of the stratification variable. Later in this chapter, we will discuss how replicate weights are used, as compared to single design weights in an example. 4.4 Characteristics of YOUR survey Survey data that come from reputable sources, such as most federal agencies or repositories such as the Inter-university Consortium for Political and Social Research (ICPSR) at the University of Michigan in the United States, are accompanied by descriptions of the data source including when and where it was collected, what its target population is, and information on the design of the survey. This will include information on sample design, such as stratum or cluster variables, and design or replicate weights to be used when you conduct your analysis. I cannot stress enough that learning how your particular survey data source is designed, and how the designers recommend you use provided survey variables for your analysis, is imperative to ensure your analysis is correctly specified. 4.5 Example from the American Community Survey Lets look at an example of these ideas in a real data source. Throughout the book I will use several complex survey design data sources to illustrate various topics, in this chapter I will use data from the US Census Bureaus American Community Survey (ACS) public use microdata sample (PUMS). We can actually use the tidycensus package (Walker and Herman 2021) to download ACS PUMS directly from the Census Bureau. This example shows how to extract the 2018 single-year PUMS for the state of Texas, and only keep variables related to person-records. The ACS has information on both people and households, but for now well only look at the person records. Help on these functions can be found by typing ?pums_variables and ?get_pums in R library(tidycensus) library(tidyverse) pums_vars_18&lt;- pums_variables %&gt;% filter(year== 2018, survey == &quot;acs1&quot;) %&gt;% distinct(var_code, var_label, data_type, level) %&gt;% filter(level == &quot;person&quot;) TX_pums &lt;- get_pums( variables = c(&quot;PUMA&quot;, &quot;SEX&quot;, &quot;AGEP&quot;, &quot;CIT&quot;, &quot;JWTR&quot;,&quot;JWRIP&quot;, &quot;HISP&quot;), state = &quot;AL&quot;, survey = &quot;acs1&quot;, year = 2018) knitr::kable( head(TX_pums), format = &#39;html&#39; ) These data are also easily available from the Integrated Public Use Microdata Series (IPUMS) project housed at the University of Minnesota (Ruggles et al. 2021). The IPUMS version of the data adds additional information to the data and homogenizes the data across multiple years to make using it easier. The following example will use the ipumsr package to read in an extract from IPUMS-USA. After you create an IPUMS extract, right click on the DDI link and save that file to your computer. Then repeat this for the .DAT file. If you need help creating an IPUMS extract, their staff have created a tutorial for doing so (https://usa.ipums.org/usa/tutorials.shtml). This will save the xml file that contains all the information on the data (what is contained in the data file) to your computer. When using IPUMS, it will have a name like usa_xxxxx.xml where the xs represent the extract number. You will also need to download the data file, by right clicking the Download.DAT link in the above image. This will save a .gz file to your computer, again with a name like: usa_xxxxx.dat.gz. Make sure this file and the xml file from above are in the same folder. The fundamentals of using ipumsr is to specify the name of your .xml file from your extract, and as long as your .tar.gz file from your extract is in the same location, R will read the data. The files on my computer: library(ipumsr) library(tidyverse) ddi &lt;- read_ipums_ddi(ddi_file = &quot;~/OneDrive - University of Texas at San Antonio/projects/book_data//usa_00097.xml&quot;) ipums &lt;- read_ipums_micro(ddi = ddi) Will read in the data, in this case, it is a subset of the 2008 to 2012 single year ACS. This extract is not all of the variables from the ACS, as that would be a very large file and for my purposes here, I dont need that. My goal for the rest of the chapter is to illustrate how to use the IPUMS as an example of a complex survey design data set and the steps necessary to do so. 4.6 Basics of analyzing survey data A fundamental part of analyzing complex survey data are knowing the variables within the data that contain the survey design information. The US Census Bureau has documented the design of the survey in a publication (US Census Bureau 2014) The IPUMS version of the ACS has two variables STRATA and CLUSTER that describe the two stage process by which the data are collected. Here are a the first few lines of these from the data: options(scipen = 999) library(knitr) kable(head(ipums[, c(&quot;SERIAL&quot;, &quot;STRATA&quot;, &quot;CLUSTER&quot;)], n=10), digits = 14 ) For the ACS, the strata variable is named, ironically STRATA and the cluster variable CLUSTER. The IPUMS creates the STRATA variable based on the sampling strata in the ACS, and the CLUSTER variable based on households within a stratum. Often in surveys, the clusters may not be households, they could be smaller population aggregates, such as neighborhoods and villages, as in the DHS. The data also come with housing unit weights and person unit weights, so your analysis can be either representative of housing units or people. kable(head(ipums[, c(&quot;SERIAL&quot;, &quot;STRATA&quot;, &quot;CLUSTER&quot;, &quot;HHWT&quot;, &quot;PERWT&quot;)], n=10), digits = 14 ) SERIAL STRATA CLUSTER HHWT PERWT 1189369 330148 2019011893691 67 67 1189370 231948 2019011893701 43 43 1189371 690048 2019011893711 114 114 1189372 430248 2019011893721 34 34 1189373 650048 2019011893731 35 35 1189374 680548 2019011893741 19 19 1189375 340048 2019011893751 18 18 1189376 60048 2019011893761 37 37 1189377 440048 2019011893771 76 76 1189378 462348 2019011893781 10 10 As can be seen in the first few cases, the HHWT variable is the same for everyone in a given household, but each person has a unique person weight showing that they each represent different numbers of people in the population. Further investigation of the housing and person weights allow us to see what these values actually look like. summary(ipums$PERWT) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 49.0 78.0 106.3 128.0 2376.0 Here we see the minimum person weight is 1 and the maximum is 2376, which tells us that at least on person in the data represents 2376 people in the population that year. A histogram of the weights can also show us the distribution of weights in the sample. library(ggplot2) ipums%&gt;% ggplot(aes(x = PERWT)) + geom_histogram() + labs(title = &quot;Histogram of ACS Person Weights, 2019&quot;) We can see how the weights inflate each person or household to the population by summing the weights. Below, I sum the person weights for the state of Texas, the sum is 28,995,881 million people, which is the same as the official estimate of the population in 2019 (https://www.census.gov/quickfacts/TX), we also see, by using the n() function, that there were 272,776 persons in the sample in 2019 living in Texas. library(dplyr) ipums%&gt;% filter(STATEFIP == 48)%&gt;% summarize(tot_pop = sum( PERWT ) , n_respond = n()) ## # A tibble: 1 x 2 ## tot_pop n_respond ## &lt;dbl&gt; &lt;int&gt; ## 1 28995881 272776 For housing units, we have to select a single person from the household in order for the same process to work, otherwise we would misrepresent the number of households in the state. We see there are 10,585,803 million housing units, and 114,016 unique households in the data. ipums%&gt;% filter(STATEFIP == 48, PERNUM == 1)%&gt;% summarize(tothh = sum( HHWT ) , n_housing = n()) ## # A tibble: 1 x 2 ## tothh n_housing ## &lt;dbl&gt; &lt;int&gt; ## 1 10585803 114016 This total is nearly identical to that from the Censuss ACS estimate. This exercise shows that by using the provided weights in the survey, we can estimate the population size the sample was supposed to capture effectively. The survey package and the newer tidyverse package srvyr are designed to fully implement survey design and weighting and perform a wide variety of statistical summaries. The way these packages work, is that you provide the name of your data frame, and the survey design variables that are in your data and the package code performs the requested analysis, correcting for survey design and weighting to the appropriate population. The code below illustrates how to enter the survey design for the IPUMS-USA ACS. Some surveys will not have both a cluster and stratification variable, so again, its important to consult your survey documentation to find these for your data. The function as_survey_design() in this case takes three arguments, since we are piping the 2019 ACS into it, we dont have to specify the name of the data. ids is the argument for the cluster variable, if your survey doesnt have one, just leave it out. strata is where you specify the name of the survey stratification variable, and weights is where you specify the name of the appropriate weighting variable. In this case, Im replicating the estimate of the housing units in Texas from above, so Ill use the HHWT variable. The easiest way to get a total population estimate is to use the survey_total() function, which is equivalent to summing the weights as shown above, although in the case of the survey analysis commands in the survey and srvyr packages, the total will also be estimated with a standard error of the estimate. library(srvyr) library(survey) ipums%&gt;% filter(STATEFIP == 48, PERNUM == 1)%&gt;% as_survey_design(cluster= CLUSTER, strata = STRATA, weights = HHWT)%&gt;% summarize(tothh = survey_total()) div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} A short aside about survey design options The core definition of the ACS survey design is shown in the code above, and I highly recommend that you inspect the help file for the survey design functions ?as_survey_design or ?svydesign. An important option that often has to be specified is the nest=TRUE option. This if often necessary if PSU identifiers are not unique across strata. For example, the fictional data shown below has the PSUs values the same across strata.} fake_survey&lt;- data.frame( strata = c(1,1,1,1,1,1, 2,2,2,2,2,2), psu = c(1,1,1,2,2,3, 1,1,2,2,3,3), weight = rpois(n = 12, lambda = 20)) knitr::kable(fake_survey) strata psu weight 1 1 14 1 1 20 1 1 24 1 2 17 1 2 20 1 3 19 2 1 14 2 1 18 2 2 28 2 2 22 2 3 20 2 3 8 If we attempt to make a survey design from this, R would show an error. fake_design&lt;- fake_survey%&gt;% as_survey_design(ids = psu, strata=strata, weights = weight) ## Error in svydesign.default(ids, probs, strata, variables, fpc, .data, : Clusters not nested in strata at top level; you may want nest=TRUE. But if we include the nest = TRUE option, R doesnt give us the error: fake_design&lt;- fake_survey%&gt;% as_survey_design(ids = psu, strata=strata, weights = weight, nest = TRUE) fake_design ## Stratified 1 - level Cluster Sampling design (with replacement) ## With (6) clusters. ## Called via srvyr ## Sampling variables: ## - ids: psu ## - strata: strata ## - weights: weight ## Data variables: strata (dbl), psu (dbl), weight (int) The ACS from IPUMS has unique CLUSTERs across strata, so we dont have to specify that argument when we declare our survey design. Back to our housing estimates. In this case, our tothh estimate is identical to summing the weights, but new we also have an estimate of the precision of the estimate, so we could produce a more informed statistical estimate that in 2019, there were 10,585,803 \\(\\pm\\) 27,274.96 occupied housing units in the state. If your data come with replicate weights instead of strata and cluster variables, this can be specified using the as_survey_rep() command instead as_survey_design(). In this case, we have to specify all of the columns which correspond to the replicate weights in the data. There are likely many ways to do this, but below, I use a method that matches the column names using a regular expression, where we are looking for the string REPWT, followed by any number of numeric digits, that is what the [0-9]+ portion tells R to do. Also, the ACS uses a balanced replicate weight construction, which also requires the case weight as well (Ruggles et al. 2021), so we specify the replicate weight type as BRR. Again, this is specific to the ACS, and you need to consult your own code book for your survey for your design information. In this case, we get the same estimate for the total number of housing units, but a smaller variance in the estimate, which is often seen when using replicate weights. ipums%&gt;% filter(STATEFIP == 48, PERNUM == 1)%&gt;% as_survey_rep(weight = HHWT, repweights =matches(&quot;REPWT[0-9]+&quot;), type = &quot;JK1&quot;, scale = 4/80, rscales = rep(1, 80), mse = TRUE)%&gt;% summarize(tothh = survey_total()) ## # A tibble: 1 x 2 ## tothh tothh_se ## &lt;dbl&gt; &lt;dbl&gt; ## 1 10585803 15479. rw&lt;-ipums%&gt;% filter(STATEFIP==48, PERNUM==1)%&gt;% select(REPWT1:REPWT50) t1&lt;-survey::svrepdesign(data=ipums[ipums$STATEFIP==48&amp;ipums$PERNUM==1,], repweights = rw, weights = ipums$HHWT[ipums$STATEFIP==48&amp;ipums$PERNUM==1], type=&quot;JK1&quot;,scale = .05, rscales = rep(1, ncol(rw)), mse= TRUE) t1$variables$ones&lt;-1 library(survey) svytotal(~ones, design=t1) ## total SE ## ones 10585803 11850 We can also define the survey design outside of a dplyr pipe if we want using the survey package. acs_design &lt;- svydesign(ids = ~ CLUSTER, strata= ~ STRATA, weights = ~ PERWT, data=ipums) acs_design ## Stratified 1 - level Cluster Sampling design (with replacement) ## With (114016) clusters. ## svydesign(ids = ~CLUSTER, strata = ~STRATA, weights = ~PERWT, ## data = ipums) Of course we typically want to do more analysis than just estimate a population size, and typically we are interested in using survey data for comparisons and regression modeling. To carry out any sort of statistical testing on survey data, we must not only weight the data appropriately but we must also calculate all measures of variability correctly as well. Since surveys are stratified, the traditional formula for variances is not correct because under stratified sampling, all estimates are not only a function of the total sample, but also the within-strata sample averages and sample sizes. We can estimate the variances in our estimates using the design variables and sample weights in the survey analysis procedures, but there are options. 4.7 Replicates and jack knifes and expansions, oh my! When conducting your analysis, you may not have any choices of whether you should use replicate weights or design weights, because your survey may only have one of these. There are two main strategies to estimate variances in survey data, the Taylor Series Approximation also referred to as linearization and the use of replicate weights. The Taylor Series, or linearization method is an approximation to the true variance, but is likely the most commonly used technique when analyzing survey data using regression methods. Lohr (2019) describes the calculation of variances from simple and clustered random samples in her book, and by her admission, once one has a clustered random sample the variance calculations for simple calculations becomes much more complex. The problem is that we often want much more complicated calculations in our work and the variance formulas for anything other than simple ratios are not analytically known. The Taylor series approximation to the variance for complex and nonlinear terms such as ratios or estimates of regression parameters. The survey package in R will do this if you specify a survey design that includes strata or clusters, while if you specify replicate weights then it will use an appropriate technique depending on how the data were collected. Typical replicate methods include balanced replicates, where there are exactly two clusters within each stratum, jackknife methods, which effectively remove one cluster from the strata and perform all calculations without that cluster in the analysis, then average across all replicates, and bootstrap methods which randomly sample clusters within strata with replacement a large number of times to get an estimate of the quantities of interest. 4.8 Descriptive analysis of survey data The survey library allows many forms of descriptive and regression analysis. 4.9 Weighted frequencies and rates Basic frequency tables are very useful tools for examining bivariate associations in survey data. In the survey analysis packages in R, the basic tools for doing this are the svytable() function in survey, or via the survey_total() function in srvyr. First I will recode two variables in the ACS, the employment status to indicate if a respondent is currently employed, and the MET2013 variable, which is the metropolitan area where the respondent was living. This will give us the ACS estimate for the employed and unemployed population in each Texas MSA. I first have to filter the data to be people of working age, who are in the labor force and living in a MSA. ipums %&gt;% filter(EMPSTAT %in% 1:2, AGE &gt;= 16 &amp; AGE &lt;= 65, MET2013 != 0) %&gt;% mutate(employed = as.factor(case_when(.$EMPSTAT == 1 ~ &quot;Employed&quot;, .$EMPSTAT == 2 ~ &quot;Unemployed&quot; )), met_name = haven::as_factor(MET2013)) %&gt;% as_survey_design(cluster = CLUSTER, strata = STRATA, weights = PERWT) %&gt;% group_by(met_name, employed)%&gt;% summarize(emp_rate = survey_total()) %&gt;% head() ## # A tibble: 6 x 4 ## # Groups: met_name [3] ## met_name employed emp_rate emp_rate_se ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Amarillo, TX Employed 114201 2824. ## 2 Amarillo, TX Unemployed 3761 843. ## 3 Austin-Round Rock, TX Employed 1193654 10735. ## 4 Austin-Round Rock, TX Unemployed 47998 3247. ## 5 Beaumont-Port Arthur, TX Employed 163936 3513. ## 6 Beaumont-Port Arthur, TX Unemployed 5851 774. This is OK, but if we want the totals in columns versus rows, we need to reshape the data. To go from the current long form of the variables to a wide form, we can use pivot_wider in dplyr. ipums %&gt;% filter(EMPSTAT %in% 1:2, AGE &gt;= 16 &amp; AGE &lt;= 65, MET2013 != 0) %&gt;% mutate(employed = as.factor(case_when(.$EMPSTAT == 1 ~ &quot;Employed&quot;, .$EMPSTAT == 2 ~ &quot;Unemployed&quot; )), met_name = haven::as_factor(MET2013)) %&gt;% as_survey_design(cluster = CLUSTER, strata = STRATA, weights = PERWT) %&gt;% group_by(met_name, employed)%&gt;% summarize(emp_rate = survey_total()) %&gt;% pivot_wider(id = met_name, names_from = employed, values_from = c(emp_rate, emp_rate_se) ) %&gt;% head() ## # A tibble: 6 x 5 ## # Groups: met_name [6] ## met_name emp_rate_Employ~ emp_rate_Unempl~ emp_rate_se_Emp~ emp_rate_se_Une~ ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Amarillo,~ 114201 3761 2824. 843. ## 2 Austin-Ro~ 1193654 47998 10735. 3247. ## 3 Beaumont-~ 163936 5851 3513. 774. ## 4 Brownsvil~ 161922 8153 3500. 1267. ## 5 College S~ 111576 3517 3132. 686. ## 6 Corpus Ch~ 208801 12365 4222. 1536. Of course, if we want rates, this would imply us having to divide these columns to calculate the rate, but we can also get R to do this for us using survey_mean(). Since the employed variable is dichotomous, if we take the mean of its various levels, we get a proportion, in this case the employment and unemployment rates, respectively. ipums %&gt;% filter(EMPSTAT %in% 1:2, AGE &gt;= 16 &amp; AGE &lt;= 65, MET2013 != 0) %&gt;% mutate(employed = as.factor(case_when(.$EMPSTAT == 1 ~ &quot;Employed&quot;, .$EMPSTAT == 2 ~ &quot;Unemployed&quot; )), met_name = haven::as_factor(MET2013)) %&gt;% as_survey_design(cluster = CLUSTER, strata = STRATA, weights = PERWT) %&gt;% group_by(met_name, employed)%&gt;% summarize(emp_rate = survey_mean()) %&gt;% pivot_wider(id = met_name, names_from = employed, values_from = c(emp_rate, emp_rate_se) ) %&gt;% head() ## # A tibble: 6 x 5 ## # Groups: met_name [6] ## met_name emp_rate_Employ~ emp_rate_Unempl~ emp_rate_se_Emp~ emp_rate_se_Une~ ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Amarillo,~ 0.968 0.0319 0.00706 0.00706 ## 2 Austin-Ro~ 0.961 0.0387 0.00259 0.00259 ## 3 Beaumont-~ 0.966 0.0345 0.00461 0.00461 ## 4 Brownsvil~ 0.952 0.0479 0.00735 0.00735 ## 5 College S~ 0.969 0.0306 0.00599 0.00599 ## 6 Corpus Ch~ 0.944 0.0559 0.00684 0.00684 Which gets us the employment rate and the unemployment rate for each metropolitan area, with their associated standard errors. This is a general process that would work well for any grouping variable. If we create an object from this calculation, in this case Ill call it tx_rates, then we can also easily feed it into ggplot to visualize the rates with their associated 95% confidence intervals. The geom_errorbar addition to a ggplot object can add errors to estimates, which are great because we convey the uncertainty in the rates. tx_rates%&gt;% ggplot()+ geom_bar(aes(x=met_name, y = emp_rate_Unemployed), stat = &quot;identity&quot;)+ geom_errorbar(aes(x=met_name, ymin=emp_rate_Unemployed-1.96*emp_rate_se_Unemployed, ymax= emp_rate_Unemployed+1.96*emp_rate_se_Unemployed), width=.25)+ scale_y_continuous(labels = scales::percent)+ labs(x = &quot;MSA&quot;, y = &quot;Unemployment Rate&quot;, title = &quot;Unmployment rate in Texas MSAs&quot;)+ theme(axis.text.x = element_text(angle = 45, hjust = 1)) We see that among the first 6 MSAs in the state (that are in the ACS microdata), Corpus Christi has the highest unemployment rate a %, and College Station-Bryan has the lowest unemployment rate at %. We can also use these functions for continuous variables, say with incomes. In the code below, I pipe all the elements of the analysis together to illustrate the workflow that we can do to calculate the median income in each MSA and plot it along with its error. ipums %&gt;% filter(EMPSTAT %in% 1:2, AGE &gt;= 16 &amp; AGE &lt;= 65, MET2013 != 0) %&gt;% mutate(income = ifelse(INCWAGE &lt;= 0, NA, INCWAGE), met_name = haven::as_factor(MET2013)) %&gt;% as_survey_design(cluster = CLUSTER, strata = STRATA, weights = PERWT) %&gt;% group_by(met_name)%&gt;% summarize(median_wage = survey_median(income, na.rm=T)) %&gt;% head() %&gt;% ggplot()+ geom_bar(aes(x=met_name, y = median_wage), stat = &quot;identity&quot;)+ geom_errorbar(aes(x=met_name, ymin=median_wage-1.96*median_wage_se, ymax= median_wage+1.96*median_wage_se), width=.25)+ scale_y_continuous(labels = scales::dollar)+ labs(x = &quot;MSA&quot;, y = &quot;Median Wage&quot;, title = &quot;Median wage in Texas MSAs&quot;)+ theme(axis.text.x = element_text(angle = 45, hjust = 1)) Which shows that Austin-Round Rock has the highest median wage and Brownsville-Harlingen has the lowest median wage. 4.10 Creating tables from survey data analysis Tabular output from our survey data analysis is possible through several different means. When using dplyr, our intermediate output of the analysis is always a data frame, and so any R method for printing data frames would work for simple tabular display. For instance, if we just use knitr::kable() on our workflow from above instead of piping into a plot we would get something like this: ipums %&gt;% filter(EMPSTAT %in% 1:2, AGE &gt;= 16 &amp; AGE &lt;= 65, MET2013 != 0) %&gt;% mutate(income = ifelse(INCWAGE &lt;= 0, NA, INCWAGE), met_name = haven::as_factor(MET2013)) %&gt;% as_survey_design(cluster = CLUSTER, strata = STRATA, weights = PERWT) %&gt;% group_by(met_name)%&gt;% summarize(median_wage = survey_median(income, na.rm=T)) %&gt;% head() %&gt;% knitr::kable(format = &quot;latex&quot;, digits = 0, caption = &quot;Median Wages in Texas MSAs&quot;, align = &#39;c&#39;, col.names =c(&quot;MSA Name&quot;, &quot;Median Wage&quot;, &quot;Median Wage SE&quot;)) Which is OK, but there are other ways to make tables for reports. The gt package (Iannone, Cheng, and Schloerke 2021) is built using tidyverse principles, and build tables in much the same way that ggplot builds plots, and fits easily into a dplyr workflow. Here, I use gt to produce a similar table to that from knitr::kable from above. library(gt, quietly = T) ipums %&gt;% filter(EMPSTAT %in% 1:2, AGE &gt;= 16 &amp; AGE &lt;= 65, MET2013 != 0) %&gt;% mutate(income = ifelse(INCWAGE &lt;= 0, NA, INCWAGE), met_name = haven::as_factor(MET2013)) %&gt;% as_survey_design(cluster = CLUSTER, strata = STRATA, weights = PERWT) %&gt;% group_by(met_name)%&gt;% summarize(median_wage = survey_median(income, na.rm=T)) %&gt;% head()%&gt;% gt() %&gt;% tab_header(title = &quot;Median Wages in Texas MSAs&quot;)%&gt;% cols_label(met_name = &quot;MSA Name&quot;, median_wage = &quot;Median Wage&quot;, median_wage_se = &quot;Median Wage SE&quot;)%&gt;% fmt_number(columns = c( median_wage, median_wage_se), decimals = 0, use_seps = TRUE) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #zlgvmykvcq .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #zlgvmykvcq .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #zlgvmykvcq .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #zlgvmykvcq .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #zlgvmykvcq .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #zlgvmykvcq .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #zlgvmykvcq .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #zlgvmykvcq .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #zlgvmykvcq .gt_column_spanner_outer:first-child { padding-left: 0; } #zlgvmykvcq .gt_column_spanner_outer:last-child { padding-right: 0; } #zlgvmykvcq .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #zlgvmykvcq .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #zlgvmykvcq .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #zlgvmykvcq .gt_from_md > :first-child { margin-top: 0; } #zlgvmykvcq .gt_from_md > :last-child { margin-bottom: 0; } #zlgvmykvcq .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #zlgvmykvcq .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #zlgvmykvcq .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #zlgvmykvcq .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #zlgvmykvcq .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #zlgvmykvcq .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #zlgvmykvcq .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #zlgvmykvcq .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #zlgvmykvcq .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #zlgvmykvcq .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #zlgvmykvcq .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #zlgvmykvcq .gt_sourcenote { font-size: 90%; padding: 4px; } #zlgvmykvcq .gt_left { text-align: left; } #zlgvmykvcq .gt_center { text-align: center; } #zlgvmykvcq .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #zlgvmykvcq .gt_font_normal { font-weight: normal; } #zlgvmykvcq .gt_font_bold { font-weight: bold; } #zlgvmykvcq .gt_font_italic { font-style: italic; } #zlgvmykvcq .gt_super { font-size: 65%; } #zlgvmykvcq .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Median Wages in Texas MSAs MSA Name Median Wage Median Wage SE Amarillo, TX 33,000 1,529 Austin-Round Rock, TX 42,000 1,020 Beaumont-Port Arthur, TX 40,000 1,784 Brownsville-Harlingen, TX 25,200 1,020 College Station-Bryan, TX 30,000 1,529 Corpus Christi, TX 34,000 1,428 In general, the gt tables are much easier to make look nice, compared to basic tables, because theyre much more customizable. The gtsummary package extends the table functionality by combining the summary functions like dplyr with the table structures of gt. Additionally, it will recognize survey design objects so that information can also be integrated into your workflow. The gtsummary presents a more descriptive statistical summary of the variables included, and actually uses dplyr tools under the hood of the package. In the code below, I first filter and mutate the IPUMS data to contain working age people who are employed, and who live in the six Texas cities featured in the examples above. I also create a new income variable that excludes all zero incomes, and drop levels of the MET2013 variable that arent in the list I specified. This pipes into the survey design function from the survey package, which pipes into the tbl_svysummary function which summarizes income for each MSA. This function has a lot of options to specify its output, and I recommend you consult the examples at the authors website3. library(gtsummary) ## #BlackLivesMatter ipums %&gt;% filter(EMPSTAT == 1, AGE &gt;= 16 &amp; AGE &lt;= 65, MET2013 != 0, MET2013 %in% c(11100, 12420, 13140, 15180, 17780, 18580)) %&gt;% mutate(income = ifelse(INCWAGE &lt;= 0, NA, INCWAGE), met_name = haven::as_factor(MET2013))%&gt;% select(met_name, income, CLUSTER, STRATA, PERWT)%&gt;% droplevels()%&gt;% survey::svydesign(id = ~ CLUSTER, strata = ~ STRATA, weights = ~ PERWT, data = .) %&gt;% tbl_svysummary(by = &quot;met_name&quot;, missing = &quot;no&quot;, include = c(met_name, income), label = list(income = &quot;Median Wage&quot;))%&gt;% as_hux_table() Table 4.1: Characteristic Amarillo, TX, N = 114,201 Austin-Round Rock, TX, N = 1,193,654 Beaumont-Port Arthur, TX, N = 163,936 Brownsville-Harlingen, TX, N = 161,922 College Station-Bryan, TX, N = 111,576 Corpus Christi, TX, N = 208,801 Median Wage34,000 (19,200, 56,000)43,000 (24,000, 75,000)40,000 (20,000, 67,000)26,000 (14,000, 47,000)30,000 (14,000, 51,111)35,000 (20,000, 58,000) Median (IQR) 4.10.1 How this differs from simple random sampling So the big question I often get from students is Do I really need to weight my analysis? and of course, I say, Of course! Weights dont just inflate your data to the population, they serve a very important role in making sure your sample data arent biased in their scope. Bias can enter into survey data in many forms, but nonresponse bias can dramatically affect population based estimates if key segments of our target population respond at low rates. Surveys will often deal with this by rigorous data collection strategies, but often the survey designers have to account for the added probability of nonresponse by modification of their basic weights. Lohr (2019) describes how this is done using a variety of methods including raking and post stratification, which typically separate the sample into subdivisions based on one or more demographic characteristic and produce weights based on how the sample deviates from the population composition. These methods are robust and are commonly used in large national surveys including the Behavioral Risk Factor Surveillance system (BRFSS). Other reasons for weighting and why it matters are oversampling. Many surveys will do this in their data collection because an important element of their target population may be small in overall size, so they will sample more respondents from that population subgroup than their proportion in the larger population would predict. For example, if a survey wanted to get detailed data on recent refugees to a country, and this group is small, say .1 percent of the overall population, they may design their study to have 10% of their survey respondents be refugees. This oversampling can be accounted for in the survey weights so the additional respondents are down-weighted to represent their fraction of the target population, while still allowing the researchers to get a large sample from this group. Surveys such as the Early Childhood Longitudinal Surveys and the National Longitudinal Study of Adolescent to Adult Health (AddHealth) routinely over-sample specific groups. For example the AddHealth over-samples black adolescents with college educated parents, Cuban, Puerto Rican, Chinese, and physically disabled adolescents. 4.10.2 How do weights affect our estimates? In the code example below, I illustrate how not including sample weights affects the estimates generated. This in effect is how traditional statistical analysis assuming random sampling would do things. So in order to compare the weighted estimates to the unweighted estimates, all we need to do is use a non-survey design oriented method to produce our estimate, and compare it to the survey design oriented method. Note that many surveys are designed to be self-weighting, so weights are not provided nor necessary, again, read the documentation for your specific survey for what it recommends. srs&lt;-ipums %&gt;% filter(EMPSTAT %in% 1:2, AGE &gt;= 16 &amp; AGE &lt;= 65, MET2013 != 0) %&gt;% mutate(employed = as.factor(case_when(.$EMPSTAT == 1 ~ &quot;Employed&quot;, .$EMPSTAT == 2 ~ &quot;Unemployed&quot; )), met_name = haven::as_factor(MET2013)) %&gt;% group_by(met_name)%&gt;% summarize(emp_rate = mean(I(employed)==&quot;Employed&quot;), emp_rate_se = sd(I(employed)==&quot;Employed&quot;)/sqrt(n()))%&gt;% # pivot_wider(id = met_name, # names_from = employed, # values_from = c(emp_rate, emp_rate_se) ) %&gt;% head() srs$estimate&lt;-&quot;Unweighted&quot; surv.est&lt;-ipums %&gt;% filter(EMPSTAT %in% 1:2, AGE &gt;= 16 &amp; AGE &lt;= 65, MET2013 != 0) %&gt;% mutate(employed = as.factor(case_when(.$EMPSTAT == 1 ~ &quot;Employed&quot;, .$EMPSTAT == 2 ~ &quot;Unemployed&quot; )), met_name = haven::as_factor(MET2013)) %&gt;% as_survey_design(cluster = CLUSTER, strata = STRATA, weights = PERWT) %&gt;% group_by(met_name)%&gt;% summarize(emp_rate = survey_mean(I(employed)==&quot;Employed&quot;)) %&gt;% #rename(emp_rate_surv = emp_rate, emp_rate_se_surv = emp_rate_se)%&gt;% head() surv.est$estimate&lt;-&quot;Weighted&quot; merged &lt;- rbind(srs, surv.est) p1&lt;-merged%&gt;% ggplot(aes(y = emp_rate, x = met_name , color = estimate))+ geom_point(stat=&quot;identity&quot;, cex=2)+ geom_line(aes(group = met_name), col = &quot;grey&quot;)+ scale_y_continuous(labels = scales::percent)+ scale_color_discrete( name = &quot;Estimate Type&quot;)%&gt;% labs(x = &quot;MSA&quot;, y = &quot;Employment Rate Estimate&quot;, title = &quot;Employment rate in Texas MSAs&quot;, subtitle = &quot;Rate estimates&quot;)+ theme(axis.text.x = element_text(angle = 45, hjust = 1)) p2&lt;-merged%&gt;% ggplot(aes(y = emp_rate_se, x = met_name , color = estimate))+ geom_point(stat=&quot;identity&quot;, cex=2)+ geom_line(aes(group = met_name), col = &quot;grey&quot;)+ scale_color_discrete()%&gt;% labs(x = &quot;MSA&quot;, y = &quot;Standard Error&quot;, title = &quot;&quot;, subtitle = &quot;Standard error of estimates&quot;)+ theme(axis.text.x = element_text(angle = 45, hjust = 1)) library(patchwork) (p1 + p2) We see that the difference in the weighted and unweighted estimates do vary by MSA, as do the standard errors of the estimates, with the survey-design calculated standard errors nearly always being higher than those assuming simple random sampling. If our data came from a survey with more extreme oversampling we would likely see larger differences in these estimates, and it is generally advisable to include both weights and survey design elements in all analysis of complex survey data. 4.11 Basic statistical testing on survey data. To perform basic bivariate statistical tests for frequency tables, the srvyr::svychisq and survey::svy_chisq are the primary tools you need. These will test for basic independence among rows and columns of a frequency table. Below is an example on how you would test for independence of labor force participation and gender in the IPUMS ACS data. ipums %&gt;% filter(EMPSTAT != 0, AGE &gt;= 16 &amp; AGE &lt;= 65) %&gt;% mutate(lab_force_part = ifelse (test = EMPSTAT %in% c(1,2), yes = 1, no = 0), gender = ifelse(test = SEX ==1, yes = &quot;Male&quot;, no = &quot;Female&quot;)) %&gt;% as_survey_design(cluster = CLUSTER, strata = STRATA, weights = PERWT) %&gt;% svychisq(~lab_force_part+gender, design = .) ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## F = 1955.4, ndf = 1, ddf = 172976, p-value &lt; 0.00000000000000022 The output above is for the survey adjusted chi square test (Rao and Scott 1984), which is actually calculated as an F-test in this case. We see a large F-test value, and a very small p value, which indicates that men and women have different labor force participation rates. Using our workflow from above, we can calculate the actual rates as well. ipums %&gt;% filter(EMPSTAT != 0, AGE &gt;= 16 &amp; AGE &lt;= 65) %&gt;% mutate(lab_force_part = ifelse (test = EMPSTAT %in% c(1,2), yes = 1, no = 0), gender = ifelse(test = SEX ==1, yes = &quot;Male&quot;, no = &quot;Female&quot;)) %&gt;% as_survey_design(cluster = CLUSTER, strata = STRATA, weights = PERWT) %&gt;% group_by(gender)%&gt;% summarize(lf_part_rate = survey_mean(lab_force_part, na.rm=T)) %&gt;% head()%&gt;% gt() %&gt;% tab_header(title = &quot;Labor Force Participation Rates in Texas&quot;)%&gt;% cols_label(gender = &quot;Gender&quot;, lf_part_rate = &quot;Labor Force Participation Rate&quot;, lf_part_rate_se = &quot;SE&quot;)%&gt;% fmt_number(columns = c( lf_part_rate, lf_part_rate_se), decimals = 3, use_seps = TRUE) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #mmvvgmfvjx .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #mmvvgmfvjx .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #mmvvgmfvjx .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #mmvvgmfvjx .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #mmvvgmfvjx .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mmvvgmfvjx .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #mmvvgmfvjx .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #mmvvgmfvjx .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #mmvvgmfvjx .gt_column_spanner_outer:first-child { padding-left: 0; } #mmvvgmfvjx .gt_column_spanner_outer:last-child { padding-right: 0; } #mmvvgmfvjx .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #mmvvgmfvjx .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #mmvvgmfvjx .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #mmvvgmfvjx .gt_from_md > :first-child { margin-top: 0; } #mmvvgmfvjx .gt_from_md > :last-child { margin-bottom: 0; } #mmvvgmfvjx .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #mmvvgmfvjx .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #mmvvgmfvjx .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #mmvvgmfvjx .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #mmvvgmfvjx .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #mmvvgmfvjx .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #mmvvgmfvjx .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #mmvvgmfvjx .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mmvvgmfvjx .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #mmvvgmfvjx .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #mmvvgmfvjx .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #mmvvgmfvjx .gt_sourcenote { font-size: 90%; padding: 4px; } #mmvvgmfvjx .gt_left { text-align: left; } #mmvvgmfvjx .gt_center { text-align: center; } #mmvvgmfvjx .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #mmvvgmfvjx .gt_font_normal { font-weight: normal; } #mmvvgmfvjx .gt_font_bold { font-weight: bold; } #mmvvgmfvjx .gt_font_italic { font-style: italic; } #mmvvgmfvjx .gt_super { font-size: 65%; } #mmvvgmfvjx .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Labor Force Participation Rates in Texas Gender Labor Force Participation Rate SE Female 0.674 0.002 Male 0.797 0.002 So we see that males have a much higher labor force participation rate, compared to females, and this puts the differences that we observed from the chi square test into better context. 4.11.1 Regression and survey design The design of our surveys affect the most basic estimates we do, and likewise, the design affects the more complicated analysis as well. Regression models are the work horse of social science research and we will spend a significant amount of the chapters that follow on thorough inspection of them. In the context of this chapter, I felt like I need to show both how to include survey design in a regression model and illustrate that weighting and survey design matters in terms of the output from out models. This section is NOT a total coverage of these models, and is at best a short example. This example will go in a different direction and use data from the Demographic and Health Survey instead of the ACS. The Demographic and Health Survey (DHS) data have been collected since the mid 1980s in over 90 countries around the world, and the DHS provides a public model data set that represents data on real households, without a specific national context. These data are provided to let people learn how to use the data before applying for access. The model data can be downloaded freely from the DHS[^surveydata-3] as a SAS or STATA format, or from my Github site[^surveydata-4] for this book. Below, I will read in the data from Github and recode child growth stunting relative to the WHO standard as an outcome[^surveydata-5] , and child age, rural residence and gender as predictors. The DHS household file is arrayed with a column for every child, so we must reshape the data from wide to long format using pivot_longer for the variables for child height relative to the WHO standard (hc70), child gender hc27, and child age hc1. The other variables are common to the household, so we do not have to reshape them (per the cols= line below). We then recode the outcome and the predictors for our regression example. dhs_model_hh &lt;- readRDS( url(&quot;https://github.com/coreysparks/data/blob/master/dhs_model_hh.rds?raw=true&quot;) ) dhs_model_hh_sub &lt;- dhs_model_hh%&gt;% select(hc27_01:hc27_20, hc70_01:hc70_20, hc1_01:hc1_20, hv021, hv025, hv270, hv005, hv021, hv022)%&gt;% pivot_longer(cols = c(-hv021, -hv025, -hv270, -hv005, -hv021, -hv022), names_to = c(&quot;.value&quot;, &quot;child&quot;), names_sep = &quot;_&quot;) %&gt;% na.omit()%&gt;% mutate(stunting = car::Recode(hc70, recodes = &quot;-900:-200 = 1; 9996:9999 = NA; else = 0&quot;), gender = ifelse(test = hc27 == 1, yes = &quot;male&quot;, no = &quot;female&quot;), hh_wealth = as.factor(hv270), age = hc1, age2 = hc1^2, rural = ifelse(test = hv025 ==2, yes = &quot;rural&quot;, no = &quot;urban&quot;), wt = hv005/1000000, psu = hv021, strata = hv022) dhs_model_des&lt;- dhs_model_hh_sub%&gt;% as_survey_design(cluster = psu, strata = strata, weights = wt, nest = TRUE) summary(dhs_model_des) ## Stratified Independent Sampling design (with replacement) ## Called via srvyr ## Probabilities: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.1552 0.7872 1.2943 1.5297 2.0336 6.6422 ## Stratum Sizes: ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## obs 33 136 52 41 159 23 108 117 185 266 218 44 244 25 125 183 152 80 144 ## design.PSU 33 136 52 41 159 23 108 117 185 266 218 44 244 25 125 183 152 80 144 ## actual.PSU 33 136 52 41 159 23 108 117 185 266 218 44 244 25 125 183 152 80 144 ## 20 21 22 23 24 25 26 27 ## obs 62 83 74 74 73 6 122 124 ## design.PSU 62 83 74 74 73 6 122 124 ## actual.PSU 62 83 74 74 73 6 122 124 ## Data variables: ## [1] &quot;hv021&quot; &quot;hv025&quot; &quot;hv270&quot; &quot;hv005&quot; &quot;hv022&quot; &quot;child&quot; ## [7] &quot;hc27&quot; &quot;hc70&quot; &quot;hc1&quot; &quot;stunting&quot; &quot;gender&quot; &quot;hh_wealth&quot; ## [13] &quot;age&quot; &quot;age2&quot; &quot;rural&quot; &quot;wt&quot; &quot;psu&quot; &quot;strata&quot; Just for completeness, I create a bar chart showing the percent stunted by the two main variables, gender and rural residence. dhs_model_des%&gt;% summarise( svyby(formula = ~ stunting, by = ~ gender + rural, design = ., FUN = svymean, na.rm=T))%&gt;% ggplot()+ geom_bar(aes(x = gender, y = stunting, fill = rural), stat=&quot;identity&quot;, position=&quot;dodge&quot;)+ labs(x = &quot;Child Gender&quot;, y = &quot;Percent Stunted&quot;, title = &quot;Percent Stunted by Gender and Rural Residence&quot;, subtitle = &quot;DHS Model Data&quot;)+ scale_y_continuous(labels = scales::percent) Now I estimate two logistic regression models for the stunting outcome. The first assumes random sampling and the second includes the full survey design information. For the unweighted regular logistic regression, we use the glm() function with family = binomial (link = \"logit\"). library(broom) m1&lt;-glm(stunting ~ age + age2 + rural + gender, data = dhs_model_hh_sub, family = binomial(link = &quot;logit&quot;)) m1%&gt;% tidy() ## # A tibble: 5 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.67 0.158 -10.6 4.47e-26 ## 2 age 0.0976 0.0111 8.82 1.10e-18 ## 3 age2 -0.00145 0.000176 -8.23 1.91e-16 ## 4 ruralurban -0.484 0.0934 -5.19 2.15e- 7 ## 5 gendermale 0.0461 0.0837 0.551 5.82e- 1 For the survey design model, you specify the design versus the data set name, otherwise the same code works just fine. m2 &lt;- dhs_model_des%&gt;% svyglm(stunting ~ age + age2 +rural + gender, design = ., family = binomial) ## Warning in eval(family$initialize): non-integer #successes in a binomial glm! m2%&gt;% tidy() ## # A tibble: 5 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.77 0.180 -9.83 2.03e-22 ## 2 age 0.108 0.0128 8.39 8.18e-17 ## 3 age2 -0.00164 0.000209 -7.84 6.77e-15 ## 4 ruralurban -0.432 0.128 -3.38 7.25e- 4 ## 5 gendermale 0.00938 0.103 0.0906 9.28e- 1 There are lots of ways to make a table from a regression model, but stargazer (Hlavac 2018) is a simple way to present multiple models side by side. stargazer::stargazer(m1, m2, keep.stat = &quot;n&quot;, model.names = F, column.labels = c(&quot;Unweighted model&quot;, &quot;Weighted Model&quot;), type = &quot;latex&quot;, header = FALSE, style = &quot;demography&quot;, title = &quot;Output from Unweighted and Weighted Regression Models&quot;) In this case, we see that the coefficient estimates are very similar between the two models, but the coefficient standard errors are all smaller in the unweighted model. This is commonly what you see in this situation, because the survey deign model is actually using clustered standard errors instead of asymptotic standard errors Ibragimov and Müller (2016). While this example does not show an extreme difference, it is commonplace for t-statistics generated from clustered standard errors to have higher p-values than those using asymptotic standard errors. As a result, if the t-statistic is lower, and the p-value higher, you can easily get differences in your hypothesis tests for regression parameters. This is always something to be aware of as you analyze survey data. [^surveydata-3] https://dhsprogram.com/data/model-datasets.cfm [^surveydata-4] https://github.com/coreysparks/dem-stats-book [^surveydata-4] Per the guide to DHS statistics 4.12 More examples from the CDC BRFSS Below I illustrate the use of survey characteristics when conducting descriptive analysis of a survey data set and a linear regression model estimated from that data. For this example I am using 2016 CDC Behavioral Risk Factor Surveillance System (BRFSS) SMART metro area survey data. Link #load brfss library(car) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## some ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode library(stargazer) ## ## Please cite as: ## Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. ## R package version 5.2.2. https://CRAN.R-project.org/package=stargazer library(survey) library(questionr) library(dplyr) load(url(&quot;https://github.com/coreysparks/data/blob/master/brfss_2017.Rdata?raw=true&quot;)) 4.12.1 Fix variable names #The names in the data are very ugly, so I make them less ugly nams&lt;-names(brfss_17) head(nams, n=10) ## [1] &quot;dispcode&quot; &quot;statere1&quot; &quot;safetime&quot; &quot;hhadult&quot; &quot;genhlth&quot; &quot;physhlth&quot; ## [7] &quot;menthlth&quot; &quot;poorhlth&quot; &quot;hlthpln1&quot; &quot;persdoc2&quot; #we see some names are lower case, some are upper and some have a little _ in the first position. This is a nightmare. newnames&lt;-tolower(gsub(pattern = &quot;_&quot;,replacement = &quot;&quot;,x = nams)) names(brfss_17)&lt;-newnames 4.12.2 Recoding of variables Be sure to always check your codebooks! #Poor or fair self rated health brfss_17$badhealth&lt;-Recode(brfss_17$genhlth, recodes=&quot;4:5=1; 1:3=0; else=NA&quot;) #sex brfss_17$male&lt;-as.factor(ifelse(brfss_17$sex==1, &quot;Male&quot;, &quot;Female&quot;)) #race/ethnicity brfss_17$black&lt;-Recode(brfss_17$racegr3, recodes=&quot;2=1; 9=NA; else=0&quot;) brfss_17$white&lt;-Recode(brfss_17$racegr3, recodes=&quot;1=1; 9=NA; else=0&quot;) brfss_17$other&lt;-Recode(brfss_17$racegr3, recodes=&quot;3:4=1; 9=NA; else=0&quot;) brfss_17$hispanic&lt;-Recode(brfss_17$racegr3, recodes=&quot;5=1; 9=NA; else=0&quot;) brfss_17$race_eth&lt;-Recode(brfss_17$racegr3, recodes=&quot;1=&#39;nhwhite&#39;; 2=&#39;nh black&#39;; 3=&#39;nh other&#39;;4=&#39;nh multirace&#39;; 5=&#39;hispanic&#39;; else=NA&quot;, as.factor = T) #insurance brfss_17$ins&lt;-Recode(brfss_17$hlthpln1, recodes =&quot;7:9=NA; 1=1;2=0&quot;) #income grouping brfss_17$inc&lt;-ifelse(brfss_17$incomg==9, NA, brfss_17$incomg) #education level brfss_17$educ&lt;-Recode(brfss_17$educa, recodes=&quot;1:2=&#39;0Prim&#39;; 3=&#39;1somehs&#39;; 4=&#39;2hsgrad&#39;; 5=&#39;3somecol&#39;; 6=&#39;4colgrad&#39;;9=NA&quot;, as.factor=T) brfss_17$educ&lt;-relevel(brfss_17$educ, ref=&#39;2hsgrad&#39;) #employment brfss_17$employ&lt;-Recode(brfss_17$employ1, recodes=&quot;1:2=&#39;Employed&#39;; 2:6=&#39;nilf&#39;; 7=&#39;retired&#39;; 8=&#39;unable&#39;; else=NA&quot;, as.factor=T) brfss_17$employ&lt;-relevel(brfss_17$employ, ref=&#39;Employed&#39;) #marital status brfss_17$marst&lt;-Recode(brfss_17$marital, recodes=&quot;1=&#39;married&#39;; 2=&#39;divorced&#39;; 3=&#39;widowed&#39;; 4=&#39;separated&#39;; 5=&#39;nm&#39;;6=&#39;cohab&#39;; else=NA&quot;, as.factor=T) brfss_17$marst&lt;-relevel(brfss_17$marst, ref=&#39;married&#39;) #Age cut into intervals brfss_17$agec&lt;-cut(brfss_17$age80, breaks=c(0,24,39,59,79,99)) #BMI, in the brfss_17a the bmi variable has 2 implied decimal places, so we must divide by 100 to get real bmi&#39;s brfss_17$bmi&lt;-brfss_17$bmi5/100 brfss_17$obese&lt;-ifelse(brfss_17$bmi&gt;=30, 1, 0) 4.12.3 Filter cases brfss_17&lt;-brfss_17%&gt;% filter(sex!=9, is.na(educ)==F) 4.12.4 Analysis First, we will do some descriptive analysis, such as means and cross tabulations. #First we will do some tables #Raw frequencies table(brfss_17$badhealth, brfss_17$educ) ## ## 2hsgrad 0Prim 1somehs 3somecol 4colgrad ## 0 42204 2657 5581 50592 87642 ## 1 13533 2412 3843 11544 9177 #column percentages 100*prop.table(table(brfss_17$badhealth, brfss_17$educ), margin=2) ## ## 2hsgrad 0Prim 1somehs 3somecol 4colgrad ## 0 75.72 52.42 59.22 81.42 90.52 ## 1 24.28 47.58 40.78 18.58 9.48 #basic chi square test of independence chisq.test(table(brfss_17$badhealth, brfss_17$educ)) ## ## Pearson&#39;s Chi-squared test ## ## data: table(brfss_17$badhealth, brfss_17$educ) ## X-squared = 12749, df = 4, p-value &lt;2e-16 So basically all of these numbers are incorrect, since they all assume random sampling. Now, we must tell R what the survey design is and what the weight variable is, then we can re-do these so they are correct. 4.12.5 Create a survey design object Now we identify the survey design. ids = PSU identifers, strata=strata identifiers, weights=sampling weights, data= the data frame where these variables are located. Lastly, I only include respondents with NON-MISSING case weights. I first try to get only cities in the state of Texas by looking for TX in the MSAs name field in the data. brfss_17$tx&lt;-NA brfss_17$tx[grep(pattern = &quot;TX&quot;, brfss_17$mmsaname)]&lt;-1 Now I make the survey design object. You may be required to specify two options here: survey.lonely.psu This means that some of the strata only have 1 PSU within them. This does not allow for within strata variance to be calculated. So we often have to tell the computer to do something here. Two valid options are adjust, to center the stratum at the population mean rather than the stratum mean, and average to replace the variance contribution of the stratum by the average variance contribution across strata. (from ?surveyoptions) Nesting of PSU within strata. By default, PSUs have numeric identifiers that can overlap between strata. By specifying nest=T, we tell R to re-lable the PSUs so they are unique across strata. If your survey requires this, it will throw a warning message. brfss_17&lt;-brfss_17%&gt;% filter(tx==1, is.na(mmsawt)==F, is.na(badhealth)==F) # options(survey.lonely.psu = &quot;adjust&quot;) des&lt;-svydesign(ids=~1, strata=~ststr, weights=~mmsawt, data = brfss_17 ) Simple weighted analysis Now , we re-do the analysis from above using only weights: #counts cat&lt;-wtd.table(brfss_17$badhealth, brfss_17$educ, weights = brfss_17$mmsawt) #proportions prop.table(wtd.table(brfss_17$badhealth, brfss_17$educ, weights = brfss_17$mmsawt), margin=2) ## 2hsgrad 0Prim 1somehs 3somecol 4colgrad ## 0 0.7579 0.6003 0.6690 0.8348 0.9115 ## 1 0.2421 0.3997 0.3310 0.1652 0.0885 #compare that with the original, unweighted proportions prop.table(table(brfss_17$badhealth, brfss_17$educ), margin=2) ## ## 2hsgrad 0Prim 1somehs 3somecol 4colgrad ## 0 0.709 0.569 0.623 0.780 0.884 ## 1 0.291 0.431 0.377 0.220 0.116 There are differences, notably that the prevalence of poor SRH is higher in the sample than the population. This is important! Lets say we also want the standard errors of these percentages. This can be found for a proportion by: \\(s.e. (p)={\\sqrt {p(1-p)} \\over {n}}\\) So we need to get n and p, thats easy: n&lt;-table(is.na(brfss_17$badhealth)==F) n TRUE 8535 p&lt;-prop.table(wtd.table(brfss_17$badhealth, brfss_17$educ, weights = brfss_17$mmsawt), margin=2) t(p) 0 1 2hsgrad 0.7579 0.2421 0Prim 0.6003 0.3997 1somehs 0.6690 0.3310 3somecol 0.8348 0.1652 4colgrad 0.9115 0.0885 p&lt;-prop.table(wtd.table(brfss_17$badhealth, brfss_17$male, weights = brfss_17$mmsawt), margin=2) t(p) 0 1 Female 0.791 0.209 Male 0.819 0.181 se&lt;-(p*(1-p))/n[2] p&lt;- knitr::kable(data.frame(proportion=p, se=sqrt(se))[, c(1,2,3,6)], format = &quot;html&quot;, caption = &quot;Frequencies assuming random sampling&quot;) Which shows us the errors in the estimates based on the weighted proportions. Thats nice, but since we basically inflated the n to be the population of the US, these standard errors are too small. This is another example of using survey statistical methods, to get the right standard error for a statistic. 4.12.6 Proper survey design analysis #Now consider the full sample design + weights cat&lt;-svyby(formula = ~badhealth, by=~male+agec+ins, design = des, FUN=svymean) svychisq(~badhealth+male, design = des) Pearson&#39;s X^2: Rao &amp; Scott adjustment data: svychisq(~badhealth + male, design = des) F = 2, ndf = 1, ddf = 8486, p-value = 0.1 knitr::kable(cat, caption = &quot;Survey Estimates of Poor SRH by Sex&quot;, align = &#39;c&#39;, format = &quot;html&quot;) Table 4.2: Survey Estimates of Poor SRH by Sex male agec ins badhealth se Female.(0,24].0 Female (0,24] 0 0.144 0.070 Male.(0,24].0 Male (0,24] 0 0.203 0.077 Female.(24,39].0 Female (24,39] 0 0.277 0.056 Male.(24,39].0 Male (24,39] 0 0.227 0.042 Female.(39,59].0 Female (39,59] 0 0.286 0.053 Male.(39,59].0 Male (39,59] 0 0.262 0.049 Female.(59,79].0 Female (59,79] 0 0.270 0.086 Male.(59,79].0 Male (59,79] 0 0.348 0.129 Female.(79,99].0 Female (79,99] 0 0.215 0.129 Male.(79,99].0 Male (79,99] 0 0.097 0.115 Female.(0,24].1 Female (0,24] 1 0.133 0.037 Male.(0,24].1 Male (0,24] 1 0.077 0.032 Female.(24,39].1 Female (24,39] 1 0.123 0.022 Male.(24,39].1 Male (24,39] 1 0.068 0.021 Female.(39,59].1 Female (39,59] 1 0.211 0.028 Male.(39,59].1 Male (39,59] 1 0.193 0.026 Female.(59,79].1 Female (59,79] 1 0.257 0.027 Male.(59,79].1 Male (59,79] 1 0.227 0.028 Female.(79,99].1 Female (79,99] 1 0.379 0.067 Male.(79,99].1 Male (79,99] 1 0.401 0.101 Which gives the same %s as the weighted table above, but we also want the correct standard errors for our bad health prevalence. The svyby() function will calculate statistics by groups, in this case we want the % in bad health by each level of education. The %s can be gotten using the svymean() function, which finds means of variables using survey design: sv.table&lt;-svyby(formula = ~badhealth, by = ~educ, design = des, FUN = svymean, na.rm=T) knitr::kable(sv.table, caption = &quot;Survey Estimates of Poor SRH by Education&quot;, align = &#39;c&#39;, format = &quot;html&quot;) Table 4.3: Survey Estimates of Poor SRH by Education educ badhealth se 2hsgrad 2hsgrad 0.242 0.019 0Prim 0Prim 0.400 0.048 1somehs 1somehs 0.331 0.043 3somecol 3somecol 0.165 0.015 4colgrad 4colgrad 0.089 0.009 #Make a survey design that is random sampling - no survey information nodes&lt;-svydesign(ids = ~1, weights = ~1, data = brfss_17) sv.table&lt;-svyby(formula = ~factor(badhealth), by = ~educ, design = nodes, FUN = svymean, na.rm=T) prop.table(table(brfss_17$sex, brfss_17$badhealth)) 0 1 1 0.3400 0.0776 2 0.4499 0.1325 knitr::kable(sv.table, caption = &quot;Estimates of Poor SRH by Education - No survey design&quot;, align = &#39;c&#39;, format = &quot;html&quot;) Table 4.3: Estimates of Poor SRH by Education - No survey design educ factor(badhealth)0 factor(badhealth)1 se.factor(badhealth)0 se.factor(badhealth)1 2hsgrad 2hsgrad 0.709 0.291 0.010 0.010 0Prim 0Prim 0.569 0.431 0.026 0.026 1somehs 1somehs 0.623 0.377 0.022 0.022 3somecol 3somecol 0.780 0.220 0.009 0.009 4colgrad 4colgrad 0.884 0.116 0.005 0.005 And we see the same point estimates of our prevalences as in the simple weighted table, but the standard errors have now been adjusted for survey design as well, so they are also correct. You also see they are much larger than the ones we computed above, which assumed random sampling. 4.12.7 Another way Theres this great R package, tableone that does this stuff very nicely and incorporates survey design too. Heres an example of using it to generate your bi-variate tests like above: library(tableone) #not using survey design t1&lt;-CreateTableOne(vars = c(&quot;educ&quot;, &quot;marst&quot;, &quot;male&quot;, &quot;badhealth&quot;),strata=&quot;race_eth&quot;, data = brfss_17) #t1&lt;-print(t1, format=&quot;p&quot;) print(t1,format=&quot;p&quot; ) ## Stratified by race_eth ## hispanic nh black nh multirace nh other ## n 1957 756 117 320 ## educ (%) ## 2hsgrad 27.5 28.7 19.7 12.5 ## 0Prim 15.7 1.3 3.4 0.9 ## 1somehs 12.6 7.4 1.7 3.1 ## 3somecol 22.6 28.6 33.3 18.8 ## 4colgrad 21.5 34.0 41.9 64.7 ## marst (%) ## married 48.0 31.9 41.9 62.3 ## cohab 6.7 1.3 5.1 1.9 ## divorced 11.4 18.0 12.0 6.6 ## nm 20.2 26.8 20.5 19.5 ## separated 5.5 5.6 6.0 1.9 ## widowed 8.1 16.4 14.5 7.9 ## male = Male (%) 41.1 35.3 41.0 54.4 ## badhealth (mean (SD)) 0.26 (0.44) 0.29 (0.46) 0.31 (0.46) 0.13 (0.34) ## Stratified by race_eth ## nhwhite p test ## n 5189 ## educ (%) &lt;0.001 ## 2hsgrad 20.3 ## 0Prim 0.6 ## 1somehs 2.7 ## 3somecol 27.1 ## 4colgrad 49.3 ## marst (%) &lt;0.001 ## married 54.6 ## cohab 2.2 ## divorced 13.1 ## nm 11.6 ## separated 1.1 ## widowed 17.3 ## male = Male (%) 42.1 &lt;0.001 ## badhealth (mean (SD)) 0.18 (0.38) &lt;0.001 #, strata = &quot;badhealth&quot;, test = T, #using survey design st1&lt;-svyCreateTableOne(vars = c(&quot;educ&quot;, &quot;marst&quot;, &quot;male&quot;, &quot;badhealth&quot;), strata = &quot;race_eth&quot;, test = T, data = des) #st1&lt;-print(st1, format=&quot;p&quot;) print(st1, format=&quot;p&quot;) ## Stratified by race_eth ## hispanic nh black nh multirace ## n 5177397.2 1978300.5 159546.0 ## educ (%) ## 2hsgrad 29.0 29.1 27.5 ## 0Prim 20.7 0.2 0.1 ## 1somehs 15.7 5.6 1.3 ## 3somecol 23.8 40.9 37.3 ## 4colgrad 10.7 24.3 33.7 ## marst (%) ## married 49.5 37.4 47.5 ## cohab 8.8 0.8 7.2 ## divorced 8.6 16.8 8.1 ## nm 26.1 34.0 18.9 ## separated 4.7 6.0 4.0 ## widowed 2.3 5.0 14.3 ## male = Male (%) 49.8 45.9 33.9 ## badhealth (mean (SD)) 0.23 (0.42) 0.22 (0.42) 0.13 (0.34) ## Stratified by race_eth ## nh other nhwhite p test ## n 1059547.6 6556786.6 ## educ (%) &lt;0.001 ## 2hsgrad 15.6 21.9 ## 0Prim 5.3 1.1 ## 1somehs 3.6 3.4 ## 3somecol 26.1 34.4 ## 4colgrad 49.4 39.3 ## marst (%) &lt;0.001 ## married 59.9 57.8 ## cohab 2.1 4.2 ## divorced 2.0 11.2 ## nm 32.4 17.6 ## separated 1.0 1.4 ## widowed 2.7 7.7 ## male = Male (%) 48.8 49.0 0.595 ## badhealth (mean (SD)) 0.12 (0.32) 0.16 (0.37) 0.001 4.13 Regression example Next we apply this logic to a regression case. First we fit the OLS model for our BMI outcome using education and age as predictors: fit1&lt;-lm(bmi~educ+agec, data=brfss_17) Next we incorporate case weights fit2&lt;-lm(bmi~educ+agec, data=brfss_17, weights = mmsawt) Now we will incorporate design effects as well: fit3&lt;-svyglm(bmi~educ+agec,des, family=gaussian) Now I make a table to show the results of the three models: stargazer(fit1, fit2, fit3, style=&quot;demography&quot;, type=&quot;html&quot;, column.labels = c(&quot;OLS&quot;, &quot;Weights Only&quot;, &quot;Survey Design&quot;), title = &quot;Regression models for BMI using survey data - BRFSS 2016&quot;, covariate.labels=c(&quot;PrimarySchool&quot;, &quot;SomeHS&quot;, &quot;SomeColl&quot;, &quot;CollGrad&quot;, &quot;Age 24-39&quot;,&quot;Age 39-59&quot; ,&quot;Age 59-79&quot;, &quot;Age 80+&quot;), keep.stat=&quot;n&quot;, model.names=F, align=T, ci=T) Regression models for BMI using survey data - BRFSS 2016 bmi OLS Weights Only Survey Design Model 1 Model 2 Model 3 PrimarySchool 0.114 -0.521 -0.521 (-0.671, 0.899) (-1.090, 0.046) (-1.800, 0.756) SomeHS -0.006 -0.699* -0.699 (-0.687, 0.674) (-1.270, -0.132) (-2.080, 0.682) SomeColl -0.456* -1.250*** -1.250** (-0.860, -0.052) (-1.620, -0.892) (-2.090, -0.417) CollGrad -1.970*** -2.460*** -2.460*** (-2.340, -1.600) (-2.840, -2.090) (-3.230, -1.700) Age 24-39 2.940*** 3.220*** 3.220*** (2.260, 3.620) (2.770, 3.670) (2.350, 4.100) Age 39-59 4.390*** 4.750*** 4.750*** (3.740, 5.030) (4.310, 5.180) (3.870, 5.620) Age 59-79 3.480*** 4.000*** 4.000*** (2.850, 4.110) (3.530, 4.480) (3.040, 4.970) Age 80+ 0.810* 0.819 0.819 (0.076, 1.540) (-0.051, 1.690) (-0.496, 2.130) Constant 26.000*** 26.200*** 26.200*** (25.400, 26.700) (25.800, 26.600) (25.300, 27.000) N 7,840 7,840 7,840 p &lt; .05; p &lt; .01; p &lt; .001 Notice, the results for the education levels are much less significant than the were with either of the other two analysis. This is because those models had standard errors for the parameters that were too small. You see all the standard errors are larger and the T statistics are smaller. Which shows the same \\(\\beta\\)s between the survey design model and the weighted model but the standard errors are larger in the survey model, so the test statistics are more conservative (smaller t statistics). While in this simple model, our overall interpretation of the effects do not change (positive effects of education, negative effects of age), it is entirely possible that they could once we include our survey design effects. It may be informative to plot the results of the models to see how different the coefficients are from one another: library(ggplot2) library(dplyr) coefs&lt;-data.frame(coefs=c(coef(fit1)[-1], coef(fit3)[-1]), mod=c(rep(&quot;Non Survey Model&quot;, 8),rep(&quot;Survey Model&quot;, 8)), effect=rep(names(coef(fit1)[-1]), 2)) coefs%&gt;% ggplot()+ geom_point(aes( x=effect, y=coefs, group=effect,color=effect, shape=mod), position=position_jitterdodge(jitter.width = 1), size=2)+ ylab(&quot;Regression Coefficient&quot;)+ xlab(&quot;Beta&quot;)+ geom_abline(intercept = 0, slope=0)+ theme(axis.text.x = element_text(angle = 45, hjust = 1))+ ggtitle(label = &quot;Comparison of Survey and Non-Survey Regression effects&quot;) Which shows us that the betas are similar but have some differences between the two models. 4.14 Creating Survey estimates for places One of the coolest ways to use the BRFSS is to calculate estimates for places, and by demographic characteristics withing places. Again, we use svyby() to do this, but now we calculate obesity rates by sex within cities. citytab&lt;-svyby(~obese, ~mmsaname, design=des,FUN = svymean, na.rm=T ) knitr::kable(citytab, type=&quot;html&quot;, digits=3,caption = &quot;Obesity Estimats for TX MSAs&quot;) Table 4.4: Obesity Estimats for TX MSAs mmsaname obese se Austin-Round Rock, TX, Metropolitan Statistical Area Austin-Round Rock, TX, Metropolitan Statistical Area 0.294 0.021 College Station-Bryan, TX, Metropolitan Statistical Area College Station-Bryan, TX, Metropolitan Statistical Area 0.308 0.052 Corpus Christi, TX, Metropolitan Statistical Area Corpus Christi, TX, Metropolitan Statistical Area 0.362 0.034 Dallas-Plano-Irving, TX, Metropolitan Division Dallas-Plano-Irving, TX, Metropolitan Division 0.286 0.025 El Paso, TX, Metropolitan Statistical Area El Paso, TX, Metropolitan Statistical Area 0.347 0.029 Fort Worth-Arlington, TX, Metropolitan Division Fort Worth-Arlington, TX, Metropolitan Division 0.367 0.028 Houston-The Woodlands-Sugar Land, TX, Metropolitan Statistical Area Houston-The Woodlands-Sugar Land, TX, Metropolitan Statistical Area 0.307 0.022 San Antonio-New Braunfels, TX, Metropolitan Statistical Area San Antonio-New Braunfels, TX, Metropolitan Statistical Area 0.308 0.033 Wichita Falls, TX, Metropolitan Statistical Area Wichita Falls, TX, Metropolitan Statistical Area 0.425 0.055 4.14.1 Using srvyr Theres a new package called srvyr that incorporates the survey analysis stuff into the dplyr universe: library(srvyr) brfsurv&lt;-brfss_17%&gt;% as_survey_design(1,strata=ststr,weights=mmsawt ) brfsurv%&gt;% group_by(mmsaname)%&gt;% summarise(obprev = survey_mean(obese, na.rm=T)) 4.15 Replicate Weights If your dataset comes with replicate weights, you have to specify the survey design slightly differently. Here is an example using the IPUMS CPS data. For this data, you can get information here, but you must consult your specific data source for the appropriate information for your data. load(url(&quot;https://github.com/coreysparks/data/blob/master/cpsmar10tx.Rdata?raw=true&quot;)) names(cpsmar10tx) ## [1] &quot;year&quot; &quot;serial&quot; &quot;hwtsupp&quot; &quot;repwt&quot; &quot;statefip&quot; &quot;metarea&quot; ## [7] &quot;foodstmp&quot; &quot;REPWT1&quot; &quot;REPWT2&quot; &quot;REPWT3&quot; &quot;REPWT4&quot; &quot;REPWT5&quot; ## [13] &quot;REPWT6&quot; &quot;REPWT7&quot; &quot;REPWT8&quot; &quot;REPWT9&quot; &quot;REPWT10&quot; &quot;REPWT11&quot; ## [19] &quot;REPWT12&quot; &quot;REPWT13&quot; &quot;REPWT14&quot; &quot;REPWT15&quot; &quot;REPWT16&quot; &quot;REPWT17&quot; ## [25] &quot;REPWT18&quot; &quot;REPWT19&quot; &quot;REPWT20&quot; &quot;REPWT21&quot; &quot;REPWT22&quot; &quot;REPWT23&quot; ## [31] &quot;REPWT24&quot; &quot;REPWT25&quot; &quot;REPWT26&quot; &quot;REPWT27&quot; &quot;REPWT28&quot; &quot;REPWT29&quot; ## [37] &quot;REPWT30&quot; &quot;REPWT31&quot; &quot;REPWT32&quot; &quot;REPWT33&quot; &quot;REPWT34&quot; &quot;REPWT35&quot; ## [43] &quot;REPWT36&quot; &quot;REPWT37&quot; &quot;REPWT38&quot; &quot;REPWT39&quot; &quot;REPWT40&quot; &quot;REPWT41&quot; ## [49] &quot;REPWT42&quot; &quot;REPWT43&quot; &quot;REPWT44&quot; &quot;REPWT45&quot; &quot;REPWT46&quot; &quot;REPWT47&quot; ## [55] &quot;REPWT48&quot; &quot;REPWT49&quot; &quot;REPWT50&quot; &quot;REPWT51&quot; &quot;REPWT52&quot; &quot;REPWT53&quot; ## [61] &quot;REPWT54&quot; &quot;REPWT55&quot; &quot;REPWT56&quot; &quot;REPWT57&quot; &quot;REPWT58&quot; &quot;REPWT59&quot; ## [67] &quot;REPWT60&quot; &quot;REPWT61&quot; &quot;REPWT62&quot; &quot;REPWT63&quot; &quot;REPWT64&quot; &quot;REPWT65&quot; ## [73] &quot;REPWT66&quot; &quot;REPWT67&quot; &quot;REPWT68&quot; &quot;REPWT69&quot; &quot;REPWT70&quot; &quot;REPWT71&quot; ## [79] &quot;REPWT72&quot; &quot;REPWT73&quot; &quot;REPWT74&quot; &quot;REPWT75&quot; &quot;REPWT76&quot; &quot;REPWT77&quot; ## [85] &quot;REPWT78&quot; &quot;REPWT79&quot; &quot;REPWT80&quot; &quot;REPWT81&quot; &quot;REPWT82&quot; &quot;REPWT83&quot; ## [91] &quot;REPWT84&quot; &quot;REPWT85&quot; &quot;REPWT86&quot; &quot;REPWT87&quot; &quot;REPWT88&quot; &quot;REPWT89&quot; ## [97] &quot;REPWT90&quot; &quot;REPWT91&quot; &quot;REPWT92&quot; &quot;REPWT93&quot; &quot;REPWT94&quot; &quot;REPWT95&quot; ## [103] &quot;REPWT96&quot; &quot;REPWT97&quot; &quot;REPWT98&quot; &quot;REPWT99&quot; &quot;REPWT100&quot; &quot;REPWT101&quot; ## [109] &quot;REPWT102&quot; &quot;REPWT103&quot; &quot;REPWT104&quot; &quot;REPWT105&quot; &quot;REPWT106&quot; &quot;REPWT107&quot; ## [115] &quot;REPWT108&quot; &quot;REPWT109&quot; &quot;REPWT110&quot; &quot;REPWT111&quot; &quot;REPWT112&quot; &quot;REPWT113&quot; ## [121] &quot;REPWT114&quot; &quot;REPWT115&quot; &quot;REPWT116&quot; &quot;REPWT117&quot; &quot;REPWT118&quot; &quot;REPWT119&quot; ## [127] &quot;REPWT120&quot; &quot;REPWT121&quot; &quot;REPWT122&quot; &quot;REPWT123&quot; &quot;REPWT124&quot; &quot;REPWT125&quot; ## [133] &quot;REPWT126&quot; &quot;REPWT127&quot; &quot;REPWT128&quot; &quot;REPWT129&quot; &quot;REPWT130&quot; &quot;REPWT131&quot; ## [139] &quot;REPWT132&quot; &quot;REPWT133&quot; &quot;REPWT134&quot; &quot;REPWT135&quot; &quot;REPWT136&quot; &quot;REPWT137&quot; ## [145] &quot;REPWT138&quot; &quot;REPWT139&quot; &quot;REPWT140&quot; &quot;REPWT141&quot; &quot;REPWT142&quot; &quot;REPWT143&quot; ## [151] &quot;REPWT144&quot; &quot;REPWT145&quot; &quot;REPWT146&quot; &quot;REPWT147&quot; &quot;REPWT148&quot; &quot;REPWT149&quot; ## [157] &quot;REPWT150&quot; &quot;REPWT151&quot; &quot;REPWT152&quot; &quot;REPWT153&quot; &quot;REPWT154&quot; &quot;REPWT155&quot; ## [163] &quot;REPWT156&quot; &quot;REPWT157&quot; &quot;REPWT158&quot; &quot;REPWT159&quot; &quot;REPWT160&quot; &quot;month&quot; ## [169] &quot;pernum&quot; &quot;wtsupp&quot; &quot;relate&quot; &quot;age&quot; &quot;sex&quot; &quot;race&quot; ## [175] &quot;marst&quot; &quot;offpov&quot; &quot;MIGRATE1&quot; So we see the replicate weights are in columns 8 through 167 in the data #simple binary outcome cpsmar10tx$poverty&lt;-ifelse(cpsmar10tx$offpov==1,1,0) # Replicate weight design - I got these details from the data source, you should too des2&lt;-svrepdesign( data = cpsmar10tx,repweights = cpsmar10tx[, c(8:167)] , weights = ~wtsupp , type=&quot;JK1&quot;, scale=.025) des2 ## Call: svrepdesign.default(data = cpsmar10tx, repweights = cpsmar10tx[, ## c(8:167)], weights = ~wtsupp, type = &quot;JK1&quot;, scale = 0.025) ## Unstratified cluster jacknife (JK1) with 160 replicates. #Without design prop.table(table(cpsmar10tx$poverty)) ## ## 0 1 ## 0.837 0.163 #with design prop.table(svytable(~poverty, design = des2)) ## poverty ## 0 1 ## 0.848 0.152 #Again, using the mean mean(cpsmar10tx$poverty) ## [1] 0.163 #Using the design. This would be an official estimate of poverty in TX in 2010: svymean(~poverty, design=des2) ## mean SE ## poverty 0.152 0.01 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
